plot.title = element_text(face = "bold", size = 22, hjust = 0.5)
)
return(loess_plot)
}
# Visualize the results
plots <- visualize_results(final_model, fracpol, response='uni_01', age_var='age', gender_var='Sex', id_var="id")
# Display the plots
ggarrange(plots$spaghetti_plot,plots$loess_plot,ncol=2)
plots$loess_plot
plots
library(lme4)
library(ggplot2)
library(dplyr)
library(ggpubr)
library(haven)
library(xtable)
#                    NULL)
fracpol <- read_sas("C:/Users/mde4023/OneDrive - Weill Cornell Medicine/0 Projects/KUL/Rare diseases/Rare diseases/Data/fracpol2.sas7bdat",
NULL)
#recode the responses
recode=function(x){
ifelse(x=='Yes',1,0)
}
fracpol$gen_01=sapply(as.vector(fracpol$gen_bin),FUN=recode)
fracpol$uni_01=sapply(as.vector(fracpol$uni_bin),FUN=recode)
fracpol$foc_01=sapply(as.vector(fracpol$foc_bin),FUN=recode)
#Sex variable
recode=function(x){
ifelse(x=='F','Female','Male')
}
fracpol$Sex=sapply(as.vector(fracpol$sex),FUN=recode)
# Define the model formula
response <- "uni_01"
poww1 <- "minus2"
poww2 <- "plus1"
formula <- as.formula(paste('as.factor(', response, ")~", "+ baseline_age + Male + mut_Other + mutPCDH19 +", poww1, "+",
paste(poww1, "*", c("baseline_age", "Male"), collapse = "+"), "+",
poww2, "+", paste(poww2, "*", c("baseline_age", "Male"), collapse = "+"), '+(1|id)'))
# Fit the initial model with less strict convergence criterion
initial_control_settings <- glmerControl(optimizer = "bobyqa", optCtrl = list(maxfun = 10000), tolPwrss = 1e-3)
initial_model <- glmer(formula, data = fracpol, family = binomial(link = "probit"), nAGQ = 1, control = initial_control_settings)
# Extract the fixed effect coefficients and random effect variances
start_vals <- list(fixef = fixef(initial_model), theta = getME(initial_model, "theta"))
# Fit the final model with stricter convergence criterion and using initial parameter estimates
final_control_settings <- glmerControl(optimizer = "bobyqa", optCtrl = list(maxfun = 10000))
final_model <- glmer(formula, data = fracpol, family = binomial(link = "probit"), nAGQ = 5, start = start_vals, control = final_control_settings)
fracpol$age=fracpol$baseline_age+fracpol$time
fracpol<-fracpol[!(is.na(fracpol$mutPCDH19)),]
# Function to create visualizations
visualize_results <- function(model, data=fracpol, response, age_var = "age", gender_var = "Sex", id_var = "id") {
# Create a new data frame with the original data and predicted values
data_with_preds <- data %>%
mutate(predicted = predict(model, type = "response"),
fitted = predict(model, re.form = NA, type = "response"))
# Spaghetti plot of predicted values
loess_plot <- ggplot(data_with_preds, aes_string(x = age_var, y = "fitted", color = gender_var)) +
geom_smooth(method = "loess", se = FALSE, size = 2) +  # Thick lines
ylim(0, 1) +
labs(title = "Predicted Probability by Age",
x = "Age",
y = "Predicted Probability") +
theme_minimal(base_size = 18) +  # Larger text
theme(
axis.title = element_text(face = "bold", size = 20),
axis.text = element_text(size = 18),
legend.title = element_text(face = "bold", size = 18),
legend.text = element_text(size = 16),
plot.title = element_text(face = "bold", size = 22, hjust = 0.5)
)
return(loess_plot)
}
# Visualize the results
plots <- visualize_results(final_model, fracpol, response='uni_01', age_var='age', gender_var='Sex', id_var="id")
plots
plots
library(lme4)
library(ggplot2)
library(dplyr)
library(ggpubr)
library(haven)
fracpol <- read_sas("C:/Users/mde4023/OneDrive - Weill Cornell Medicine/0 Projects/KUL/Rare diseases/Rare diseases/Data/fracpol2.sas7bdat",
NULL)
#recode the responses
recode=function(x){
ifelse(x=='Yes',1,0)
}
fracpol$gen_01=sapply(as.vector(fracpol$gen_bin),FUN=recode)
fracpol$uni_01=sapply(as.vector(fracpol$uni_bin),FUN=recode)
fracpol$foc_01=sapply(as.vector(fracpol$foc_bin),FUN=recode)
#Sex variable
recode=function(x){
ifelse(x=='F','Female','Male')
}
fracpol$Sex=sapply(as.vector(fracpol$sex),FUN=recode)
# Define the model formula
fracpol$log_plushalf=fracpol$log*fracpol$plushalf
response <- "gen_01"
poww1 <- "plushalf"
poww2 <- "log_plushalf"
formula <- as.formula(paste('as.factor(', response, ")~", "+ baseline_age + Male + mut_Other + mutPCDH19 +", poww1, "+",
paste(poww1, "*", c("baseline_age", "Male"), collapse = "+"), "+",
poww2, "+", paste(poww2, "*", c("baseline_age", "Male"), collapse = "+"), '+(1|id)'))
# Fit the initial model with less strict convergence criterion
initial_control_settings <- glmerControl(optimizer = "bobyqa", optCtrl = list(maxfun = 100000), tolPwrss = 1e-3)
initial_model <- glmer(formula, data = fracpol, family = binomial(link = "probit"), nAGQ = 1, control = initial_control_settings)
# Extract the fixed effect coefficients and random effect variances
#initial_model=final_model
start_vals <- list(fixef = fixef(initial_model), theta = getME(initial_model, "theta"))
# Fit the final model with stricter convergence criterion and using initial parameter estimates
final_control_settings <- glmerControl(optimizer = "bobyqa", optCtrl = list(maxfun = 100000))
final_model <- glmer(formula, data = fracpol, family = binomial(link = "probit"), nAGQ = 5, start = start_vals, control = final_control_settings)
fracpol$age=fracpol$baseline_age+fracpol$time
fracpol<-fracpol[!(is.na(fracpol$mutPCDH19)),]
# Function to create visualizations
visualize_results <- function(model, data=fracpol, response, age_var = "age", gender_var = "Sex", id_var = "id") {
# Create a new data frame with the original data and predicted values
data_with_preds <- data %>%
mutate(predicted = predict(model, type = "response"),
fitted = predict(model, re.form = NA, type = "response"))
# Spaghetti plot of predicted values
loess_plot <- ggplot(data_with_preds, aes_string(x = age_var, y = "fitted", color = gender_var)) +
geom_smooth(method = "loess", se = FALSE, size = 2) +  # Thick lines
ylim(0, 1) +
labs(title = "Predicted Probability by Age",
x = "Age",
y = "Predicted Probability") +
theme_minimal(base_size = 18) +  # Larger text
theme(
axis.title = element_text(face = "bold", size = 20),
axis.text = element_text(size = 18),
legend.title = element_text(face = "bold", size = 18),
legend.text = element_text(size = 16),
plot.title = element_text(face = "bold", size = 22, hjust = 0.5)
)
return(loess_plot)
}
# Visualize the results
plots <- visualize_results(final_model, fracpol, response='gen_01', age_var='age', gender_var='Sex', id_var="id")
plots$loess_plot
# Display the plots
ggarrange(plots$spaghetti_plot,plots$loess_plot,ncol=2)
plots
4563/365
3650/365
1825/365
8/1.6
96/5
64/3.5
i=1
0.05/(5-i+1)
i=2
0.05/(5-i+1)
i=3
0.05/(5-i+1)
i=4
0.05/(5-i+1)
i=5
0.05/(5-i+1)
i=5
0.05/(5-i+1)
i=1
0.05/(5-i+1)
90-(18.25+19.35+28.25)
### High dimension linear model
rm(list = ls())
#mywd='C:/Users/mde4023/OneDrive - Weill Cornell Medicine/0 Projects/FDR_Datasplitting'
mywd='C:/Users/mde4023/Documents/GitHub/FDR_Datasplitting'
setwd(mywd)
source(paste0(mywd,'/Functions/HelperFunctions.R'))
source(paste0(mywd,'/Functions/TriangleBoosterHD2.R'))
source(paste0(mywd,'/Functions Dai/knockoff.R'))
source(paste0(mywd,'/Functions Dai/analysis.R'))
source(paste0(mywd,'/Functions Dai/MBHq.R'))
source(paste0(mywd,'/Functions Dai/DS.R'))
source(paste0(mywd,'/Functions Dai/fdp_power.R'))
#devtools::install_github("Jeremy690/DSfdr/DSfdr",force = TRUE)
library(xgboost)
library(MASS)
library(glmnet)
library(knockoff)
library(mvtnorm)
library(hdi)
### algorithmic settings
num_split <- 1
n <-1500
p <- 2000
p0 <- 25
q <- 0.1
#set.seed(124)(123) i=5
set.seed(456)
signal_index <- sample(c(1:p), size = p0, replace = F)
#######set up the method for the comparison############# i=10
Compare_SignalStrength <- function(i, s,myeta = 0.05,mymax_depth = 1,mylambda = 0.5,myalpha = 0.5) {
set.seed(s)
delta <- i
# simulate data
X <- mvrnorm(n, mu = rep(0, p), Sigma = diag(p))
beta_star <- numeric(p)
beta_star[signal_index] <- rnorm(p0, 0, delta*sqrt(log(p)/n))*100
y <- scale(X^2 %*% beta_star + rnorm(n))
# run your custom methods
g1 <- ApplyTriangleBoostHD2(X = X, y = y, q = q, num_split = num_split,
signal_index = signal_index, myseed = 1,
myeta = myeta,mymax_depth = mymax_depth,mylambda = mylambda,myalpha = myalpha)
# FDR methods
# DS_result      <- DS(          X = X, y = y, q = q, num_split = num_split)
# knockoff_result<- knockoff(    X = X, y = y, q = q)
# BH_result      <- MBHq(        X = X, y = y, q = q, num_split = num_split)
# init empty results df
ResultsDataFrame <- data.frame(
Method = character(),
Delta  = numeric(),
FDP    = numeric(),
Power  = numeric(),
stringsAsFactors = FALSE
)
# bind all rows
ResultsDataFrame <- rbind(
ResultsDataFrame,
data.frame(Method = "Boost DS",                Delta = i, FDP = g1$DS_fdp,    Power = g1$DS_power),
data.frame(Method = "Boost MS",                Delta = i, FDP = g1$MDS_fdp,   Power = g1$MDS_power)#,
#data.frame(Method = "DataSplitting",           Delta = i, FDP = DS_result$DS_fdp,  Power = DS_result$DS_power),
# data.frame(Method = "MultipleDataSplitting",   Delta = i, FDP = DS_result$MDS_fdp, Power = DS_result$MDS_power),
#data.frame(Method = "Knockoff",                Delta = i, FDP = knockoff_result$fdp, Power = knockoff_result$power),
#data.frame(Method = "Benjamini–Hochberg (BH)", Delta = i, FDP = BH_result$fdp,     Power = BH_result$power)
)
return(ResultsDataFrame)
}
Compare_SignalStrength(2,7,myeta=0.2,mymax_depth = 1,mylambda = 0.5,myalpha=0.5)
Compare_SignalStrength(2,7,myeta=0.2,mymax_depth = 1,mylambda = 0.5,myalpha=50)
# define grid
#param_grid <- expand.grid(
# myeta=0.2,mymax_depth = 1,mylambda = 0.1,myalpha=0.1
param_grid <- expand.grid(
eta = c(0.1,0.2,0.3),
max_depth = c(1),
lambda = c(0.01,0.05,0.1,0.2),
alpha = c(0.01,0.05,0.1,0.2)
)
# define grid
#param_grid <- expand.grid(
# myeta=0.2,mymax_depth = 1,mylambda = 0.1,myalpha=0.1
param_grid <- expand.grid(
eta = c(0.01, 0.05, 0.1),                   # learning rate
max_depth = c(2, 4, 6),                     # shallow trees prevent overfitting
lambda = c(1, 5, 10),                       # L2 regularization
alpha = c(0, 1, 5),                         # L1 regularization
rate_drop = c(0.1, 0.3, 0.5),               # dropout rate
skip_drop = c(0.0, 0.1),                    # chance to skip dropout
nrounds = 500                               # fixed for now; can tune separately
)
results <- data.frame()
library(caret)
library(xgboost)
### algorithmic settings
num_split <- 1
n <-1500
p <- 2000
p0 <- 25
q <- 0.1
#sample the data
set.seed(456)
signal_index <- sample(c(1:p), size = p0, replace = F)
delta <- 10
X <- mvrnorm(n, mu = rep(0, p), Sigma = diag(p))
beta_star <- numeric(p)
beta_star[signal_index] <- rnorm(p0, 0, delta*sqrt(log(p)/n))*100
y <- scale(X^2 %*% beta_star + rnorm(n))
#regular start method
amountTrain=0.333
amountTest=1-amountTrain
data<-data.frame(cbind(y,X))
n <- dim(X)[1]; p <- dim(X)[2]
inclusion_rate <- matrix(0, nrow = num_split, ncol = p)
fdp <- rep(0, num_split)
power <- rep(0, num_split)
num_select <- rep(0, num_split)
data<-data.frame(cbind(y,X))
train_index<-sample(x = c(1:n), size = amountTrain * n, replace = F)
dataTrain<-data[train_index,]
colnames(dataTrain)<-c('y',paste0('X',1:p))
colnames(data)<-c('y',paste0('X',1:p))
Xtrain=X[train_index,]
names(Xtrain)=paste0('X',1:p)
remaining_index<-c(setdiff(c(1:n),train_index))
sample_index1 <- sample(x = remaining_index, size = amountTest/2 * n, replace = F)
dtrain <- xgb.DMatrix(data = as.matrix(Xtrain), label = y[train_index])
dtest <- xgb.DMatrix(data = as.matrix(X[sample_index1, ]))
y_true <- y[sample_index1]
# define grid
#param_grid <- expand.grid(
# myeta=0.2,mymax_depth = 1,mylambda = 0.1,myalpha=0.1
param_grid <- expand.grid(
eta = c(0.01, 0.05, 0.1),                   # learning rate
max_depth = c(2, 4, 6),                     # shallow trees prevent overfitting
lambda = c(1, 5, 10),                       # L2 regularization
alpha = c(0, 1, 5),                         # L1 regularization
rate_drop = c(0.1, 0.3, 0.5),               # dropout rate
skip_drop = c(0.0, 0.1),                    # chance to skip dropout
nrounds = 500                               # fixed for now; can tune separately
)
results <- data.frame()
# Run grid search
for (i in 1:nrow(param_grid)) {
params <- param_grid[i, ]
model <- xgboost(
data = dtrain,
booster = "dart",
objective = "reg:squarederror",
nrounds = 500,
eta = params$eta,
max_depth = params$max_depth,
lambda = params$lambda,
alpha = params$alpha,
rate_drop = params$rate_drop,          # dropout rate for dart
skip_drop = params$skip_drop,          # probability to skip dropout
# subsample = 0.7,
# colsample_bytree = 0.7,
verbose = 0
)
preds <- predict(model, newdata = dtest)
R2 <- 1 - sum((y_true - preds)^2) / sum((y_true - mean(y_true))^2)
results <- rbind(results, cbind(params, R2))
print(cbind(params, R2))
}
# define grid
#param_grid <- expand.grid(
# myeta=0.2,mymax_depth = 1,mylambda = 0.1,myalpha=0.1
param_grid <- expand.grid(
eta = c(0.1,0.2,0.3),
max_depth = c(1),
lambda = c(0.01,0.05,0.1,0.2),
alpha = c(0.01,0.05,0.1,0.2)
)
results <- data.frame()
# Run grid search
for (i in 1:nrow(param_grid)) {
params <- param_grid[i, ]
model <- xgboost(
data = dtrain,
booster = "gbtree",
objective = "reg:squarederror",
nrounds = 500,
eta = params$eta,
max_depth = params$max_depth,
lambda = params$lambda,
alpha = params$alpha,
#  subsample = 0.7,
#  colsample_bytree = 0.7,
verbose = 0
)
preds <- predict(model, newdata = dtest)
R2 <- 1 - sum((y_true - preds)^2) / sum((y_true - mean(y_true))^2)
results <- rbind(results, cbind(params, R2))
print(cbind(params, R2))
}
Compare_SignalStrength(2,7,myeta=0.1,mymax_depth = 1,mylambda = 0.01,myalpha=0.01)
# define grid
#param_grid <- expand.grid(
# myeta=0.2,mymax_depth = 1,mylambda = 0.1,myalpha=0.1
param_grid <- expand.grid(
eta = c(0.1,0.2,0.3),
max_depth = c(1),
lambda = c(0.01,0.05,0.1,0.2),
alpha = c(0.01,0.05,0.1,0.2)
)
results <- data.frame()
# Run grid search
for (i in 1:nrow(param_grid)) {
params <- param_grid[i, ]
model <- xgboost(
data = dtrain,
booster = "gbtree",
objective = "reg:squarederror",
nrounds = 500,
eta = params$eta,
max_depth = params$max_depth,
lambda = params$lambda,
alpha = params$alpha,
#  subsample = 0.7,
#  colsample_bytree = 0.7,
verbose = 0
)
preds <- predict(model, newdata = dtest)
R2 <- 1 - sum((y_true - preds)^2) / sum((y_true - mean(y_true))^2)
results <- rbind(results, cbind(params, R2))
print(cbind(params, R2))
}
Compare_SignalStrength(7,7,myeta=0.2,mymax_depth = 1,mylambda = 0.05,myalpha=0.05)
### High dimension linear model
rm(list = ls())
#mywd='C:/Users/mde4023/OneDrive - Weill Cornell Medicine/0 Projects/FDR_Datasplitting'
mywd='C:/Users/mde4023/Documents/GitHub/FDR_Datasplitting'
setwd(mywd)
source(paste0(mywd,'/Functions/HelperFunctions.R'))
source(paste0(mywd,'/Functions/TriangleBoosterHD2.R'))
source(paste0(mywd,'/Functions Dai/knockoff.R'))
source(paste0(mywd,'/Functions Dai/analysis.R'))
source(paste0(mywd,'/Functions Dai/MBHq.R'))
source(paste0(mywd,'/Functions Dai/DS.R'))
source(paste0(mywd,'/Functions Dai/fdp_power.R'))
#devtools::install_github("Jeremy690/DSfdr/DSfdr",force = TRUE)
library(xgboost)
library(MASS)
library(glmnet)
library(knockoff)
library(mvtnorm)
library(hdi)
### algorithmic settings
num_split <- 1
n <-2000
p <- 2500
p0 <- 25
q <- 0.1
#set.seed(124)(123) i=5
set.seed(456)
signal_index <- sample(c(1:p), size = p0, replace = F)
#######set up the method for the comparison############# i=10
Compare_SignalStrength <- function(i, s,myeta = 0.05,mymax_depth = 1,mylambda = 0.5,myalpha = 0.5) {
set.seed(s)
delta <- i
# simulate data
X <- mvrnorm(n, mu = rep(0, p), Sigma = diag(p))
beta_star <- numeric(p)
beta_star[signal_index] <- rnorm(p0, 0, delta*sqrt(log(p)/n))*100
y <- scale(X^2 %*% beta_star + rnorm(n))
# run your custom methods
g1 <- ApplyTriangleBoostHD2(X = X, y = y, q = q, num_split = num_split,
signal_index = signal_index, myseed = 1,
myeta = myeta,mymax_depth = mymax_depth,mylambda = mylambda,myalpha = myalpha)
# FDR methods
# DS_result      <- DS(          X = X, y = y, q = q, num_split = num_split)
# knockoff_result<- knockoff(    X = X, y = y, q = q)
# BH_result      <- MBHq(        X = X, y = y, q = q, num_split = num_split)
# init empty results df
ResultsDataFrame <- data.frame(
Method = character(),
Delta  = numeric(),
FDP    = numeric(),
Power  = numeric(),
stringsAsFactors = FALSE
)
# bind all rows
ResultsDataFrame <- rbind(
ResultsDataFrame,
data.frame(Method = "Boost DS",                Delta = i, FDP = g1$DS_fdp,    Power = g1$DS_power),
data.frame(Method = "Boost MS",                Delta = i, FDP = g1$MDS_fdp,   Power = g1$MDS_power)#,
#data.frame(Method = "DataSplitting",           Delta = i, FDP = DS_result$DS_fdp,  Power = DS_result$DS_power),
# data.frame(Method = "MultipleDataSplitting",   Delta = i, FDP = DS_result$MDS_fdp, Power = DS_result$MDS_power),
#data.frame(Method = "Knockoff",                Delta = i, FDP = knockoff_result$fdp, Power = knockoff_result$power),
#data.frame(Method = "Benjamini–Hochberg (BH)", Delta = i, FDP = BH_result$fdp,     Power = BH_result$power)
)
return(ResultsDataFrame)
}
Compare_SignalStrength(7,7,myeta=0.2,mymax_depth = 1,mylambda = 0.05,myalpha=0.05)
Compare_SignalStrength(13,13,myeta=0.2,mymax_depth = 1,mylambda = 0.05,myalpha=0.05)
Compare_SignalStrength(13,13,myeta=0.2,mymax_depth = 5,mylambda = 0.05,myalpha=0.05)
library(lightgbm)
install.packages("lightgbm")
library(lightgbm)
# define grid
#param_grid <- expand.grid(
# myeta=0.2,mymax_depth = 1,mylambda = 0.1,myalpha=0.1
train_data <- lgb.Dataset(data = data.matrix(Xtrain), label = y[train_index])
num_split <- 1
n <-1500
p <- 2000
p0 <- 25
q <- 0.1
#sample the data
set.seed(456)
signal_index <- sample(c(1:p), size = p0, replace = F)
delta <- 10
X <- mvrnorm(n, mu = rep(0, p), Sigma = diag(p))
beta_star <- numeric(p)
beta_star[signal_index] <- rnorm(p0, 0, delta*sqrt(log(p)/n))*100
y <- scale(X^2 %*% beta_star + rnorm(n))
#regular start method
amountTrain=0.333
amountTest=1-amountTrain
data<-data.frame(cbind(y,X))
n <- dim(X)[1]; p <- dim(X)[2]
inclusion_rate <- matrix(0, nrow = num_split, ncol = p)
fdp <- rep(0, num_split)
power <- rep(0, num_split)
num_select <- rep(0, num_split)
data<-data.frame(cbind(y,X))
train_index<-sample(x = c(1:n), size = amountTrain * n, replace = F)
dataTrain<-data[train_index,]
colnames(dataTrain)<-c('y',paste0('X',1:p))
colnames(data)<-c('y',paste0('X',1:p))
Xtrain=X[train_index,]
names(Xtrain)=paste0('X',1:p)
remaining_index<-c(setdiff(c(1:n),train_index))
sample_index1 <- sample(x = remaining_index, size = amountTest/2 * n, replace = F)
dtrain <- xgb.DMatrix(data = as.matrix(Xtrain), label = y[train_index])
dtest <- xgb.DMatrix(data = as.matrix(X[sample_index1, ]))
y_true <- y[sample_index1]
# define grid
#param_grid <- expand.grid(
# myeta=0.2,mymax_depth = 1,mylambda = 0.1,myalpha=0.1
train_data <- lgb.Dataset(data = data.matrix(Xtrain), label = y[train_index])
# Set parameters
params <- list(
objective = "regression",
metric = "rmse",  # or "l2"
learning_rate = params$eta,
max_depth = params$max_depth,
lambda_l2 = params$lambda,
lambda_l1 = params$alpha,
verbosity = -1  # suppress output
)
