FDP    = numeric(),
Power  = numeric(),
stringsAsFactors = FALSE
)
# bind all rows
ResultsDataFrame <- rbind(
ResultsDataFrame,
data.frame(Method = "Boost DS",                Delta = i, FDP = g1$DS_fdp,    Power = g1$DS_power),
data.frame(Method = "Boost MS",                Delta = i, FDP = g1$MDS_fdp,   Power = g1$MDS_power),
data.frame(Method = "DataSplitting",           Delta = i, FDP = DS_result$DS_fdp,  Power = DS_result$DS_power),
data.frame(Method = "MultipleDataSplitting",   Delta = i, FDP = DS_result$MDS_fdp, Power = DS_result$MDS_power),
data.frame(Method = "Knockoff",                Delta = i, FDP = knockoff_result$fdp, Power = knockoff_result$power),
data.frame(Method = "Benjamini–Hochberg (BH)", Delta = i, FDP = BH_result$fdp,     Power = BH_result$power)
)
return(ResultsDataFrame)
}
Compare_SignalStrength(10,8)
# --- your fixed settings ------------------------
set.seed(456)
num_split <- 50
n <-100
p <- 150
p0 <- 10
q <- 0.1
signal_index <- sample(c(1:p), size = p0, replace = F)
delta=10
# simulate data
n1 <- floor(n/2); n2 <- n - n1
X1 <- matrix(rnorm(n1*p, mean= 1), n1, p)
X2 <- matrix(rnorm(n2*p, mean=-1), n2, p)
X  <- rbind(X1, X2)
beta_star <- numeric(p)
beta_star[signal_index] <- rnorm(p0, 0, delta*sqrt(log(p)/n))*10
y <- (X^2 %*% beta_star + rnorm(n))
nrounds    <- 500
num_split  <- 1
amountTrain<- 0.333
# assume X (n×p) and y (length n) already exist
data <- data.frame(y = y, X)
# --- define a grid of candidate params -----------
param_grid <- expand.grid(
eta               = c(0.01, 0.05, 0.1),
max_depth         = c(3, 4, 6),
subsample         = c(0.6, 0.8, 1.0),
colsample_bytree  = c(0.6, 0.8, 1.0),
lambda            = c(0,0.1, 0.5, 1, 5),
alpha             = c(0,0.1, 0.5, 1),
booster           = "gbtree",
stringsAsFactors  = FALSE
)
# prepare storage
param_grid$mean_R2 <- NA_real_
# helper to compute R2
calc_r2 <- function(obs, pred) {
1 - sum((obs - pred)^2) / sum((obs - mean(obs))^2)
}
# --- grid search over params ----------------------
for(j in 1:nrow(param_grid)) {
pars   <- as.list(param_grid[j, c("eta","max_depth","subsample",
"colsample_bytree","lambda","alpha",
"booster")])
R2_vals <- numeric(num_split)
for(iter in seq_len(num_split)) {
# split train/test
train_idx <- sample(seq_len(nrow(data)), size = amountTrain * nrow(data))
test_idx  <- setdiff(seq_len(nrow(data)), train_idx)
dtrain <- xgb.DMatrix(
data  = as.matrix(data[train_idx, -1]),
label = data$y[train_idx]
)
dtest  <- xgb.DMatrix(
data  = as.matrix(data[test_idx,  -1]),
label = data$y[test_idx]
)
# fit model
bst <- xgb.train(
params   = pars,
data     = dtrain,
nrounds  = nrounds,
verbose  = 0
)
# predict & compute R2
pred         <- predict(bst, dtest)
R2_vals[iter] <- calc_r2(data$y[test_idx], pred)
}
# store average R2
param_grid$mean_R2[j] <- mean(R2_vals)
print(j)
}
# --- pick best -------------------------------
best_row   <- which.max(param_grid$mean_R2)
best_param <- param_grid[best_row, ]
print(best_param)
library(xgboost)
# --- your fixed settings ------------------------
set.seed(456)
num_split <- 50
n <-200
p <- 250
p0 <- 10
q <- 0.1
signal_index <- sample(c(1:p), size = p0, replace = F)
delta=10
# simulate data
n1 <- floor(n/2); n2 <- n - n1
X1 <- matrix(rnorm(n1*p, mean= 1), n1, p)
X2 <- matrix(rnorm(n2*p, mean=-1), n2, p)
X  <- rbind(X1, X2)
beta_star <- numeric(p)
beta_star[signal_index] <- rnorm(p0, 0, delta*sqrt(log(p)/n))*10
y <- (X^2 %*% beta_star + rnorm(n))
nrounds    <- 500
num_split  <- 1
amountTrain<- 0.333
# assume X (n×p) and y (length n) already exist
data <- data.frame(y = y, X)
# --- define a grid of candidate params -----------
param_grid <- expand.grid(
eta               = c(0.01, 0.05, 0.1),
max_depth         = c(3, 4, 6),
subsample         = c(0.6, 0.8, 1.0),
colsample_bytree  = c(0.6, 0.8, 1.0),
lambda            = c(0,0.1, 0.5, 1, 5),
alpha             = c(0,0.1, 0.5, 1),
booster           = "gbtree",
stringsAsFactors  = FALSE
)
# prepare storage
param_grid$mean_R2 <- NA_real_
# helper to compute R2
calc_r2 <- function(obs, pred) {
1 - sum((obs - pred)^2) / sum((obs - mean(obs))^2)
}
# --- grid search over params ----------------------
for(j in 1:nrow(param_grid)) {
pars   <- as.list(param_grid[j, c("eta","max_depth","subsample",
"colsample_bytree","lambda","alpha",
"booster")])
R2_vals <- numeric(num_split)
for(iter in seq_len(num_split)) {
# split train/test
train_idx <- sample(seq_len(nrow(data)), size = amountTrain * nrow(data))
test_idx  <- setdiff(seq_len(nrow(data)), train_idx)
dtrain <- xgb.DMatrix(
data  = as.matrix(data[train_idx, -1]),
label = data$y[train_idx]
)
dtest  <- xgb.DMatrix(
data  = as.matrix(data[test_idx,  -1]),
label = data$y[test_idx]
)
# fit model
bst <- xgb.train(
params   = pars,
data     = dtrain,
nrounds  = nrounds,
verbose  = 0
)
# predict & compute R2
pred         <- predict(bst, dtest)
R2_vals[iter] <- calc_r2(data$y[test_idx], pred)
}
# store average R2
param_grid$mean_R2[j] <- mean(R2_vals)
print(j)
}
# --- pick best -------------------------------
best_row   <- which.max(param_grid$mean_R2)
best_param <- param_grid[best_row, ]
print(best_param)
View(param_grid)
set.seed(456)
num_split <- 5
n <-100
p <- 150
p0 <- 10
q <- 0.1
signal_index <- sample(c(1:p), size = p0, replace = F)
delta=10
# simulate data
n1 <- floor(n/2); n2 <- n - n1
X1 <- matrix(rnorm(n1*p, mean= 1), n1, p)
X2 <- matrix(rnorm(n2*p, mean=-1), n2, p)
X  <- rbind(X1, X2)
beta_star <- numeric(p)
beta_star[signal_index] <- rnorm(p0, 0, delta*sqrt(log(p)/n))*10
y <- (X^2 %*% beta_star + rnorm(n))
nrounds    <- 500
num_split  <- 1
amountTrain<- 0.333
# assume X (n×p) and y (length n) already exist
data <- data.frame(y = y, X)
# --- define a grid of candidate params -----------
param_grid <- expand.grid(
eta               = c(0.01, 0.05, 0.1,0.3),
max_depth         = c(3, 4),
subsample         = c(0.6, 0.8, 1.0),
colsample_bytree  = c(0.6, 0.8, 1.0),
lambda            = c(0,0.1, 0.5, 1, 5),
alpha             = c(0,0.1, 0.5, 1),
booster           = "gbtree",
stringsAsFactors  = FALSE
)
# prepare storage
param_grid$mean_R2 <- NA_real_
# helper to compute R2
calc_r2 <- function(obs, pred) {
1 - sum((obs - pred)^2) / sum((obs - mean(obs))^2)
}
# --- grid search over params ----------------------
for(j in 1:nrow(param_grid)) {
pars   <- as.list(param_grid[j, c("eta","max_depth","subsample",
"colsample_bytree","lambda","alpha",
"booster")])
R2_vals <- numeric(num_split)
for(iter in seq_len(num_split)) {
# split train/test
train_idx <- sample(seq_len(nrow(data)), size = amountTrain * nrow(data))
test_idx  <- setdiff(seq_len(nrow(data)), train_idx)
dtrain <- xgb.DMatrix(
data  = as.matrix(data[train_idx, -1]),
label = data$y[train_idx]
)
dtest  <- xgb.DMatrix(
data  = as.matrix(data[test_idx,  -1]),
label = data$y[test_idx]
)
# fit model
bst <- xgb.train(
params   = pars,
data     = dtrain,
nrounds  = nrounds,
verbose  = 0
)
# predict & compute R2
pred         <- predict(bst, dtest)
R2_vals[iter] <- calc_r2(data$y[test_idx], pred)
}
# store average R2
param_grid$mean_R2[j] <- mean(R2_vals)
print(j)
}
# --- pick best -------------------------------
best_row   <- which.max(param_grid$mean_R2)
best_param <- param_grid[best_row, ]
print(best_param)
View(param_grid)
set.seed(456)
num_split <- 5
n <-400
p <- 500
p0 <- 25
q <- 0.1
signal_index <- sample(c(1:p), size = p0, replace = F)
delta=10
# simulate data
n1 <- floor(n/2); n2 <- n - n1
X1 <- matrix(rnorm(n1*p, mean= 1), n1, p)
X2 <- matrix(rnorm(n2*p, mean=-1), n2, p)
X  <- rbind(X1, X2)
beta_star <- numeric(p)
beta_star[signal_index] <- rnorm(p0, 0, delta*sqrt(log(p)/n))*10
y <- (X^2 %*% beta_star + rnorm(n))
nrounds    <- 500
num_split  <- 1
amountTrain<- 0.333
# assume X (n×p) and y (length n) already exist
data <- data.frame(y = y, X)
# --- define a grid of candidate params -----------
param_grid <- expand.grid(
eta               = c(0.01, 0.05, 0.1,0.3),
max_depth         = c(3, 4),
subsample         = c(0.6, 0.8, 1.0),
colsample_bytree  = c(0.6, 0.8, 1.0),
lambda            = c(0,0.1, 0.5, 1, 5),
alpha             = c(0,0.1, 0.5, 1),
booster           = "gbtree",
stringsAsFactors  = FALSE
)
# prepare storage
param_grid$mean_R2 <- NA_real_
# helper to compute R2
calc_r2 <- function(obs, pred) {
1 - sum((obs - pred)^2) / sum((obs - mean(obs))^2)
}
# --- grid search over params ----------------------
for(j in 1:nrow(param_grid)) {
pars   <- as.list(param_grid[j, c("eta","max_depth","subsample",
"colsample_bytree","lambda","alpha",
"booster")])
R2_vals <- numeric(num_split)
for(iter in seq_len(num_split)) {
# split train/test
train_idx <- sample(seq_len(nrow(data)), size = amountTrain * nrow(data))
test_idx  <- setdiff(seq_len(nrow(data)), train_idx)
dtrain <- xgb.DMatrix(
data  = as.matrix(data[train_idx, -1]),
label = data$y[train_idx]
)
dtest  <- xgb.DMatrix(
data  = as.matrix(data[test_idx,  -1]),
label = data$y[test_idx]
)
# fit model
bst <- xgb.train(
params   = pars,
data     = dtrain,
nrounds  = nrounds,
verbose  = 0
)
# predict & compute R2
pred         <- predict(bst, dtest)
R2_vals[iter] <- calc_r2(data$y[test_idx], pred)
}
# store average R2
param_grid$mean_R2[j] <- mean(R2_vals)
print(j)
}
# --- pick best -------------------------------
best_row   <- which.max(param_grid$mean_R2)
best_param <- param_grid[best_row, ]
print(best_param)
View(param_grid)
param_grid <- expand.grid(
eta               = c(0.01, 0.05, 0.1,0.3),
max_depth         = c(4,5,6,8),
subsample         = c(0.6, 0.8, 1.0),
colsample_bytree  = c(0.6, 0.8, 1.0),
lambda            = c(0,0.1, 0.5, 1, 5),
alpha             = c(0,0.1, 0.5, 1,1.5),
booster           = "gbtree",
stringsAsFactors  = FALSE
)
# prepare storage
param_grid$mean_R2 <- NA_real_
# helper to compute R2
calc_r2 <- function(obs, pred) {
1 - sum((obs - pred)^2) / sum((obs - mean(obs))^2)
}
# --- grid search over params ----------------------
for(j in 1:nrow(param_grid)) {
pars   <- as.list(param_grid[j, c("eta","max_depth","subsample",
"colsample_bytree","lambda","alpha",
"booster")])
R2_vals <- numeric(num_split)
for(iter in seq_len(num_split)) {
# split train/test
train_idx <- sample(seq_len(nrow(data)), size = amountTrain * nrow(data))
test_idx  <- setdiff(seq_len(nrow(data)), train_idx)
dtrain <- xgb.DMatrix(
data  = as.matrix(data[train_idx, -1]),
label = data$y[train_idx]
)
dtest  <- xgb.DMatrix(
data  = as.matrix(data[test_idx,  -1]),
label = data$y[test_idx]
)
# fit model
bst <- xgb.train(
params   = pars,
data     = dtrain,
nrounds  = nrounds,
verbose  = 0
)
# predict & compute R2
pred         <- predict(bst, dtest)
R2_vals[iter] <- calc_r2(data$y[test_idx], pred)
}
# store average R2
param_grid$mean_R2[j] <- mean(R2_vals)
print(j)
}
# --- pick best -------------------------------
best_row   <- which.max(param_grid$mean_R2)
best_param <- param_grid[best_row, ]
print(best_param)
View(param_grid)
###choose the parameters
params =list(
objective = "reg:squarederror",
eta       = 0.05,
max_depth = 4,
subsample = 0.8,
colsample_bytree = 1,
lambda    = 0.5,
alpha     = 0
)
params
#######set up the method for the comparison############# i=10 s=10 num_split=1
Compare_SignalStrength <- function(i, s) {
set.seed(s)
delta <- i
# signal_index <- sample(c(1:p), size = p0, replace = F)
# simulate data
n1 <- floor(n/2); n2 <- n - n1
X1 <- matrix(rnorm(n1*p, mean= 1), n1, p)
X2 <- matrix(rnorm(n2*p, mean=-1), n2, p)
X  <- rbind(X1, X2)
beta_star <- numeric(p)
beta_star[signal_index] <- rnorm(p0, 0, delta*sqrt(log(p)/n))*10
y <- (X^2 %*% beta_star + rnorm(n))
# run your custom methods
g1 <- ApplyTriangleBoostTrain( X = X, y = y, q = q, num_split = num_split,param=params,
signal_index = signal_index, myseed = 1)
# FDR methods
DS_result      <- DS(          X = X, y = y, q = q, num_split = num_split)
knockoff_result<- ApplyGBMKnockoff(    X = X, y = y, q = q,param=params)
BH_result      <- MBHq(        X = X, y = y, q = q, num_split = num_split)
# init empty results df
ResultsDataFrame <- data.frame(
Method = character(),
Delta  = numeric(),
FDP    = numeric(),
Power  = numeric(),
stringsAsFactors = FALSE
)
# bind all rows
ResultsDataFrame <- rbind(
ResultsDataFrame,
data.frame(Method = "Boost DS",                Delta = i, FDP = g1$DS_fdp,    Power = g1$DS_power),
data.frame(Method = "Boost MS",                Delta = i, FDP = g1$MDS_fdp,   Power = g1$MDS_power),
data.frame(Method = "DataSplitting",           Delta = i, FDP = DS_result$DS_fdp,  Power = DS_result$DS_power),
data.frame(Method = "MultipleDataSplitting",   Delta = i, FDP = DS_result$MDS_fdp, Power = DS_result$MDS_power),
data.frame(Method = "Knockoff",                Delta = i, FDP = knockoff_result$fdp, Power = knockoff_result$power),
data.frame(Method = "Benjamini–Hochberg (BH)", Delta = i, FDP = BH_result$fdp,     Power = BH_result$power)
)
return(ResultsDataFrame)
}
Compare_SignalStrength(10,8)
### High dimension linear model
rm(list = ls())
mywd='C:/Users/mde4023/Downloads/FDR_Datasplitting'
#mywd='C:/Users/mde4023/Documents/GitHub/FDR_Datasplitting'
setwd(mywd)
source(paste0(mywd,'/Functions/HelperFunctions.R'))
source(paste0(mywd,'/Functions/TriangleBoosterTrainMS.R'))
source(paste0(mywd,'/Functions/ApplyGBMKnockoff.R'))
#source(paste0(mywd,'/Functions/TriangleGBMTrainMS.R'))
source(paste0(mywd,'/Functions Dai/knockoff.R'))
source(paste0(mywd,'/Functions Dai/analysis.R'))
source(paste0(mywd,'/Functions Dai/MBHq.R'))
source(paste0(mywd,'/Functions Dai/DS.R'))
source(paste0(mywd,'/Functions Dai/fdp_power.R'))
#devtools::install_github("Jeremy690/DSfdr/DSfdr",force = TRUE)
library(xgboost)
library(gbm)
library(MASS)
library(glmnet)
library(knockoff)
library(mvtnorm)
library(hdi)
### algorithmic settings
num_split <- 50
n <-400
p <- 500
p0 <- 10
q <- 0.1
#set.seed(124)(123) i=5
set.seed(456)
signal_index <- sample(c(1:p), size = p0, replace = F)
###choose the parameters
params =list(
objective = "reg:squarederror",
eta       = 0.05,
max_depth = 4,
subsample = 0.8,
colsample_bytree = 1,
lambda    = 0.5,
alpha     = 0
)
set.seed(s)
#######set up the method for the comparison############# i=10 s=10 num_split=1
Compare_SignalStrength <- function(i, s) {
set.seed(s)
delta <- i
# signal_index <- sample(c(1:p), size = p0, replace = F)
# simulate data
n1 <- floor(n/2); n2 <- n - n1
X1 <- matrix(rnorm(n1*p, mean= 1), n1, p)
X2 <- matrix(rnorm(n2*p, mean=-1), n2, p)
X  <- rbind(X1, X2)
beta_star <- numeric(p)
beta_star[signal_index] <- rnorm(p0, 0, delta*sqrt(log(p)/n))*10
y <- (X^2 %*% beta_star + rnorm(n))
# run your custom methods
g1 <- ApplyTriangleBoostTrain( X = X, y = y, q = q, num_split = num_split,param=params,
signal_index = signal_index, myseed = 1)
# FDR methods
DS_result      <- DS(          X = X, y = y, q = q, num_split = num_split)
knockoff_result<- ApplyGBMKnockoff(    X = X, y = y, q = q,param=params)
BH_result      <- MBHq(        X = X, y = y, q = q, num_split = num_split)
# init empty results df
ResultsDataFrame <- data.frame(
Method = character(),
Delta  = numeric(),
FDP    = numeric(),
Power  = numeric(),
stringsAsFactors = FALSE
)
# bind all rows
ResultsDataFrame <- rbind(
ResultsDataFrame,
data.frame(Method = "Boost DS",                Delta = i, FDP = g1$DS_fdp,    Power = g1$DS_power),
data.frame(Method = "Boost MS",                Delta = i, FDP = g1$MDS_fdp,   Power = g1$MDS_power),
data.frame(Method = "DataSplitting",           Delta = i, FDP = DS_result$DS_fdp,  Power = DS_result$DS_power),
data.frame(Method = "MultipleDataSplitting",   Delta = i, FDP = DS_result$MDS_fdp, Power = DS_result$MDS_power),
data.frame(Method = "Knockoff",                Delta = i, FDP = knockoff_result$fdp, Power = knockoff_result$power),
data.frame(Method = "Benjamini–Hochberg (BH)", Delta = i, FDP = BH_result$fdp,     Power = BH_result$power)
)
return(ResultsDataFrame)
}
Compare_SignalStrength(10,8)
