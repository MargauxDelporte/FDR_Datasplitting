sum(fulldata$event)/length(fulldata$event)
summary(subset(fulldata,event==1)$time )
# Predictors
all_vars <- names(train_df)[1:14]
# Create the new formula
formula_str <- paste("Surv(time, event) ~", paste(all_vars, collapse = " + "))
myformula <- as.formula(formula_str)
#create the stacking
CreateStack=function(trainData,nrep,myseed){
set.seed(myseed)
stacked_df <- do.call(rbind, replicate(nrep, trainData, simplify = FALSE))
replicated_df<-rbind(stacked_df,trainData)
maxTime<-max(trainData$time[trainData$event==1])
timeCens<-runif(nrow(stacked_df),0,maxTime)
timeComp<-timeCens<stacked_df$time
stacked_df$time[timeComp]<-timeCens[timeComp]
stacked_df$event<-ifelse(timeComp,
rep(0,nrow(stacked_df)),
stacked_df$event)
stacked_df<-rbind(stacked_df,trainData)
return(list(
replicated_df=replicated_df,
stacked_df=stacked_df,
trainData=trainData
))
}
seed_R=123
set_seed(seed_R, seed_np = seed_R, seed_torch = seed_R)
mod=deepsurv(data=train_df,
time_variable = "time",
status_variable = "event")
mod1 <- predict(mod, type="risk",newdata = test_df)
UnoC(test_df$time,test_df$event,mod1)
#analysis
#{"learning_rate": 0.023094096518941305, "dropout": 0.017243652343750002, "lr_decay": 0.0009819482421875,
#"momentum": 0.926554443359375, "L2_reg": 2.364680908203125, "batch_norm": false,
#"standardize": true, "n_in": 6, "hidden_layers_sizes": [26, 26, 26], "activation": "selu"}
simFunction<-function(seedNum,nrep){
DataSeg_n=CreateStack(train_df,nrep =nrep,myseed=seedNum)
DataSeg_1=CreateStack(train_df,nrep =1,myseed=seedNum)
seed_R=seedNum
set_seed(seed_R, seed_np = seed_R, seed_torch = seed_R)
dnnModel1 <-deepsurv(data=DataSeg_n$trainData,
time_variable = "time",
status_variable = "event",
learning_rate=0.0020065103592061526,
lr_decay= 0.000645986328125,
momentum=0.8109013671875,
#L2_reg= 2.364680908203125,
frac=0.5,
activation="selu",
num_nodes=c(42, 42, 42),
dropout= 0.034404296875000004,
early_stopping=TRUE,
epochs=1000L,
patience=50L,
batch_norm = F,
batch_size=250L,
shuffle=TRUE)
dnnModel2 <-deepsurv(data=DataSeg_n$replicated_df,
time_variable = "time",
status_variable = "event",
learning_rate=0.0020065103592061526,
lr_decay= 0.000645986328125,
momentum=0.8109013671875,
#L2_reg= 2.364680908203125,
frac=0.5,
activation="selu",
num_nodes=c(42, 42, 42),
dropout= 0.034404296875000004,
early_stopping=TRUE,
epochs=1000L,
patience=50L,
batch_norm = F,
batch_size=250L,
shuffle=TRUE)
dnnModel3 <-deepsurv(data=DataSeg_n$stacked_df,
time_variable = "time",
status_variable = "event",
learning_rate=0.0020065103592061526,
lr_decay= 0.000645986328125,
momentum=0.8109013671875,
#L2_reg= 2.364680908203125,
frac=0.5,
activation="selu",
num_nodes=c(42, 42, 42),
dropout= 0.034404296875000004,
early_stopping=TRUE,
epochs=1000L,
patience=50L,
batch_norm = F,
batch_size=250L,
shuffle=TRUE)
dnnModel4 <-deepsurv(data=DataSeg_1$replicated_df,
time_variable = "time",
status_variable = "event",
learning_rate=0.0020065103592061526,
lr_decay= 0.000645986328125,
momentum=0.8109013671875,
#L2_reg= 2.364680908203125,
frac=0.5,
activation="selu",
num_nodes=c(42, 42, 42),
dropout= 0.034404296875000004,
early_stopping=TRUE,
epochs=1000L,
patience=50L,
batch_norm = F,
batch_size=250L,
shuffle=TRUE)
dnnModel5 <-deepsurv(data=DataSeg_1$stacked_df,
time_variable = "time",
status_variable = "event",
learning_rate=0.0020065103592061526,
lr_decay= 0.000645986328125,
momentum=0.8109013671875,
#L2_reg= 2.364680908203125,
frac=0.5,
activation="selu",
num_nodes=c(42, 42, 42),
dropout= 0.034404296875000004,
early_stopping=TRUE,
epochs=1000L,
patience=50L,
batch_norm = F,
batch_size=250L,
shuffle=TRUE)
dnnPred1 <- predict(dnnModel1, type="risk",newdata = test_df)
dnnPred2 <- predict(dnnModel2, type="risk",newdata = test_df)
dnnPred3 <- predict(dnnModel3, type="risk",newdata = test_df)
dnnPred4 <- predict(dnnModel4, type="risk",newdata = test_df)
dnnPred5 <- predict(dnnModel5, type="risk",newdata = test_df)
dnnC1<-UnoC(test_df$time,test_df$event,dnnPred1)
dnnC2<-UnoC(test_df$time,test_df$event,dnnPred2)
dnnC3<-UnoC(test_df$time,test_df$event,dnnPred3)
dnnC4<-UnoC(test_df$time,test_df$event,dnnPred4)
dnnC5<-UnoC(test_df$time,test_df$event,dnnPred5)
coxmd=coxph(myformula,data=DataSeg_n$trainData)
coxmdPred1 <- predict(coxmd, type="risk",newdata = test_df)
coxmdC1<-UnoC(test_df$time,test_df$event,coxmdPred1)
r=c(seedNum,dnnC1,dnnC2,dnnC3,dnnC4,dnnC5,coxmdC1)
return(c(seedNum,dnnC1,dnnC2,dnnC3,dnnC4,dnnC5,coxmdC1))
}
round(simFunction(123,10),3)
train_df
names(train_df)
# Predictors
all_vars <- names(train_df)[1:9]
# Create the new formula
formula_str <- paste("Surv(time, event) ~", paste(all_vars, collapse = " + "))
myformula <- as.formula(formula_str)
#create the stacking
CreateStack=function(trainData,nrep,myseed){
set.seed(myseed)
stacked_df <- do.call(rbind, replicate(nrep, trainData, simplify = FALSE))
replicated_df<-rbind(stacked_df,trainData)
maxTime<-max(trainData$time[trainData$event==1])
timeCens<-runif(nrow(stacked_df),0,maxTime)
timeComp<-timeCens<stacked_df$time
stacked_df$time[timeComp]<-timeCens[timeComp]
stacked_df$event<-ifelse(timeComp,
rep(0,nrow(stacked_df)),
stacked_df$event)
stacked_df<-rbind(stacked_df,trainData)
return(list(
replicated_df=replicated_df,
stacked_df=stacked_df,
trainData=trainData
))
}
seed_R=123
set_seed(seed_R, seed_np = seed_R, seed_torch = seed_R)
mod=deepsurv(data=train_df,
time_variable = "time",
status_variable = "event")
mod1 <- predict(mod, type="risk",newdata = test_df)
UnoC(test_df$time,test_df$event,mod1)
#analysis
#{"learning_rate": 0.023094096518941305, "dropout": 0.017243652343750002, "lr_decay": 0.0009819482421875,
#"momentum": 0.926554443359375, "L2_reg": 2.364680908203125, "batch_norm": false,
#"standardize": true, "n_in": 6, "hidden_layers_sizes": [26, 26, 26], "activation": "selu"}
simFunction<-function(seedNum,nrep){
DataSeg_n=CreateStack(train_df,nrep =nrep,myseed=seedNum)
DataSeg_1=CreateStack(train_df,nrep =1,myseed=seedNum)
seed_R=seedNum
set_seed(seed_R, seed_np = seed_R, seed_torch = seed_R)
dnnModel1 <-deepsurv(data=DataSeg_n$trainData,
time_variable = "time",
status_variable = "event",
learning_rate=0.0020065103592061526,
lr_decay= 0.000645986328125,
momentum=0.8109013671875,
#L2_reg= 2.364680908203125,
frac=0.5,
activation="selu",
num_nodes=c(42, 42, 42),
dropout= 0.034404296875000004,
early_stopping=TRUE,
epochs=1000L,
patience=50L,
batch_norm = F,
batch_size=250L,
shuffle=TRUE)
dnnModel2 <-deepsurv(data=DataSeg_n$replicated_df,
time_variable = "time",
status_variable = "event",
learning_rate=0.0020065103592061526,
lr_decay= 0.000645986328125,
momentum=0.8109013671875,
#L2_reg= 2.364680908203125,
frac=0.5,
activation="selu",
num_nodes=c(42, 42, 42),
dropout= 0.034404296875000004,
early_stopping=TRUE,
epochs=1000L,
patience=50L,
batch_norm = F,
batch_size=250L,
shuffle=TRUE)
dnnModel3 <-deepsurv(data=DataSeg_n$stacked_df,
time_variable = "time",
status_variable = "event",
learning_rate=0.0020065103592061526,
lr_decay= 0.000645986328125,
momentum=0.8109013671875,
#L2_reg= 2.364680908203125,
frac=0.5,
activation="selu",
num_nodes=c(42, 42, 42),
dropout= 0.034404296875000004,
early_stopping=TRUE,
epochs=1000L,
patience=50L,
batch_norm = F,
batch_size=250L,
shuffle=TRUE)
dnnModel4 <-deepsurv(data=DataSeg_1$replicated_df,
time_variable = "time",
status_variable = "event",
learning_rate=0.0020065103592061526,
lr_decay= 0.000645986328125,
momentum=0.8109013671875,
#L2_reg= 2.364680908203125,
frac=0.5,
activation="selu",
num_nodes=c(42, 42, 42),
dropout= 0.034404296875000004,
early_stopping=TRUE,
epochs=1000L,
patience=50L,
batch_norm = F,
batch_size=250L,
shuffle=TRUE)
dnnModel5 <-deepsurv(data=DataSeg_1$stacked_df,
time_variable = "time",
status_variable = "event",
learning_rate=0.0020065103592061526,
lr_decay= 0.000645986328125,
momentum=0.8109013671875,
#L2_reg= 2.364680908203125,
frac=0.5,
activation="selu",
num_nodes=c(42, 42, 42),
dropout= 0.034404296875000004,
early_stopping=TRUE,
epochs=1000L,
patience=50L,
batch_norm = F,
batch_size=250L,
shuffle=TRUE)
dnnPred1 <- predict(dnnModel1, type="risk",newdata = test_df)
dnnPred2 <- predict(dnnModel2, type="risk",newdata = test_df)
dnnPred3 <- predict(dnnModel3, type="risk",newdata = test_df)
dnnPred4 <- predict(dnnModel4, type="risk",newdata = test_df)
dnnPred5 <- predict(dnnModel5, type="risk",newdata = test_df)
dnnC1<-UnoC(test_df$time,test_df$event,dnnPred1)
dnnC2<-UnoC(test_df$time,test_df$event,dnnPred2)
dnnC3<-UnoC(test_df$time,test_df$event,dnnPred3)
dnnC4<-UnoC(test_df$time,test_df$event,dnnPred4)
dnnC5<-UnoC(test_df$time,test_df$event,dnnPred5)
coxmd=coxph(myformula,data=DataSeg_n$trainData)
coxmdPred1 <- predict(coxmd, type="risk",newdata = test_df)
coxmdC1<-UnoC(test_df$time,test_df$event,coxmdPred1)
r=c(seedNum,dnnC1,dnnC2,dnnC3,dnnC4,dnnC5,coxmdC1)
return(c(seedNum,dnnC1,dnnC2,dnnC3,dnnC4,dnnC5,coxmdC1))
}
round(simFunction(123,10),3)
power_result <- pwr.t.test(n = 200, d = 2.81, sig.level = 0.05, type = "two.sample", alternative = "two.sided")
# Load necessary libraries
#install.packages("effsize")
#install.packages("pwr")
library(readxl)
library(effsize)
power_result <- pwr.t.test(n = 200, d = 2.81, sig.level = 0.05, type = "two.sample", alternative = "two.sided")
# Load necessary libraries
#install.packages("effsize")
#install.packages("pwr")
library(readxl)
library(effsize)
library(pwr)
power_result <- pwr.t.test(n = 200, d = 2.81, sig.level = 0.05, type = "two.sample", alternative = "two.sided")
power_result
required_n <- pwr.t.test(power = 0.80, d = 2.81, sig.level = 0.05, type = "two.sample", alternative = "two.sided")
required_n
?pwr.t.test
pwr.t.test(n = 200, d = 2.81, sig.level = 0.05, type = "two.sample", alternative = "two.sided")
pwr.t.test(n = 200, d = 2.81, sig.level = 0.05, type = "two.sample", alternative = "two.sided")
power_result <- pwr.t.test(n = 200, d = 0.281, sig.level = 0.05, type = "two.sample", alternative = "two.sided")
power_result
library(glmnet)
mydata2
# Load necessary libraries
#install.packages("effsize")
#install.packages("pwr")
library(readxl)
library(effsize)
library(pwr)
#read in the data
SupMaterial <- read_excel("C:/Users/mde4023/OneDrive - Weill Cornell Medicine/0 Projects/Project Rulla/PowerCalc/SupMaterial.xlsx",
sheet = "S27-ClinicalFeatures")
Hips <- read_excel("C:/Users/mde4023/OneDrive - Weill Cornell Medicine/0 Projects/Project Rulla/PowerCalc/SupMaterial.xlsx",
sheet = "S27-TCGA_HiPSScores")
#define the subgroups
threshold=10
SupMaterial$subgroups=ifelse(SupMaterial[,2]==0&SupMaterial[,3]<threshold,NA, #follow up less then threshold
ifelse((SupMaterial[,3]>threshold&SupMaterial[,2]==0)| #censored after more then 10 years
(SupMaterial[,3]>15&SupMaterial[,2]==1),'A', #or died after 15 years
ifelse(SupMaterial[,3]<15&SupMaterial[,2]==1,'B',NA))) #died before 15 years
table(SupMaterial$subgroups)
#check the subgroups
subsetA=subset(SupMaterial,subgroups=='A')[,c(1,2,3)]
View(subsetA)
whas500
file <- H5File$new("C:/Users/mde4023/Downloads/StackedSurvivalData/WHAS/whas_train_test.h5", mode = "r")
file
# simulation parameters
n        <- 200    # sample size
p        <- 240     # number of features
s        <- 20      # number of non-zero signals
beta_val <- 1       # signal magnitude
M        <- 50      # number of splits in MDS
q        <- 0.1     # target FDR
# 1. simulate X from 0.5*N(2,I) + 0.5*N(-2,I)
n1 <- floor(n/2); n2 <- n - n1
X1 <- matrix(rnorm(n1*p, mean= 2), n1, p)
X2 <- matrix(rnorm(n2*p, mean=-2), n2, p)
X  <- rbind(X1, X2)
# 2. sparse beta and response
true_idx <- sample(p, s)
true_idx
beta_val
beta     <- numeric(p); beta[true_idx] <- beta_val
beta
y        <- X %*% beta + rnorm(n)
y
### High dimension linear model
rm(list = ls())
mywd='C:/Users/mde4023/Downloads/FDR_Datasplitting'
#mywd='C:/Users/mde4023/Documents/GitHub/FDR_Datasplitting'
setwd(mywd)
source(paste0(mywd,'/Functions/HelperFunctions.R'))
source(paste0(mywd,'/Functions/TriangleBoosterTrainMS.R'))
source(paste0(mywd,'/Functions/ApplyGBMKnockoff.R'))
#source(paste0(mywd,'/Functions/TriangleGBMTrainMS.R'))
source(paste0(mywd,'/Functions Dai/knockoff.R'))
source(paste0(mywd,'/Functions Dai/analysis.R'))
source(paste0(mywd,'/Functions Dai/MBHq.R'))
source(paste0(mywd,'/Functions Dai/DS.R'))
source(paste0(mywd,'/Functions Dai/fdp_power.R'))
#devtools::install_github("Jeremy690/DSfdr/DSfdr",force = TRUE)
library(xgboost)
library(gbm)
library(ranger)
library(MASS)
library(neuralnet)
library(glmnet)
library(knockoff)
library(mvtnorm)
library(hdi)
### algorithmic settings
num_split <- 50
n <-500
p <- 100
p0 <- 10
q <- 0.1
#set.seed(124)(123) i=5
set.seed(456)
signal_index <- sample(c(1:p), size = p0, replace = F)
###choose the parameters
params =list(
objective = "reg:squarederror",
eta       = 0.05,
max_depth = 3,
subsample = 0.8,
colsample_bytree = 0.8,
lambda    = 0.15,
alpha     = 0.15
)
#######set up the method for the comparison############# i=10 s=10 num_split=1
Compare_SignalStrength <- function(i, s) {
set.seed(s)
delta <- i
signal_index <- sample(c(1:p), size = p0, replace = F)
# simulate data
n1 <- floor(n/2); n2 <- n - n1
X1 <- matrix(rnorm(n1*p, mean= 1), n1, p)
X2 <- matrix(rnorm(n2*p, mean=-1), n2, p)
X  <- rbind(X1, X2)
beta_star <- numeric(p)
beta_star[signal_index] <- rnorm(p0, 0, delta*sqrt(log(p)/n))*10
y <- (X^2 %*% beta_star + rnorm(n))
# run your custom methods
g1 <- ApplyTriangleBoostTrain( X = X, y = y, q = q, num_split = num_split,param=params,
signal_index = signal_index, myseed = 1)
# FDR methods
DS_result      <- DS(          X = X, y = y, q = q, num_split = num_split)
knockoff_result<- ApplyGBMKnockoff(    X = X, y = y, q = q,param=params)
BH_result      <- MBHq(        X = X, y = y, q = q, num_split = num_split)
# init empty results df
ResultsDataFrame <- data.frame(
Method = character(),
Delta  = numeric(),
FDP    = numeric(),
Power  = numeric(),
stringsAsFactors = FALSE
)
# bind all rows
ResultsDataFrame <- rbind(
ResultsDataFrame,
data.frame(Method = "Boost DS",                Delta = i, FDP = g1$DS_fdp,    Power = g1$DS_power),
data.frame(Method = "Boost MS",                Delta = i, FDP = g1$MDS_fdp,   Power = g1$MDS_power),
data.frame(Method = "DataSplitting",           Delta = i, FDP = DS_result$DS_fdp,  Power = DS_result$DS_power),
data.frame(Method = "MultipleDataSplitting",   Delta = i, FDP = DS_result$MDS_fdp, Power = DS_result$MDS_power),
data.frame(Method = "Knockoff",                Delta = i, FDP = knockoff_result$fdp, Power = knockoff_result$power),
data.frame(Method = "Benjamini–Hochberg (BH)", Delta = i, FDP = BH_result$fdp,     Power = BH_result$power)
)
return(ResultsDataFrame)
}
Compare_SignalStrength(10,8)
#######run the code#############
#Results=data.frame()
#for(s in 1:25){
#  for(i in seq(from=5,to=13,by=1)){
#  Results=rbind(Results,Compare_SignalStrength(i,s))
#  print(s)
#  }
#  print(Results)
#  }
library(parallel)
mywd='C:/Users/mde4023/Downloads/FDR_Datasplitting'
setwd(mywd)
# Source helper and method files
source(file.path(mywd, 'Functions', 'TriangleBoosterTrainMS.R'))
source(file.path(mywd, 'Functions', 'HelperFunctions.R'))
source(file.path(mywd, 'Functions', 'ApplyGBMKnockoff.R'))
# Dai’s routines
#source(file.path(mywd, 'Functions Dai', 'knockoff.R'))
source(file.path(mywd, 'Functions Dai', 'analysis.R'))
source(file.path(mywd, 'Functions Dai', 'MBHq.R'))
source(file.path(mywd, 'Functions Dai', 'DS.R'))
source(file.path(mywd, 'Functions Dai', 'fdp_power.R'))
# Load required packages
pkgs <- c('xgboost','gbm','MASS','glmnet','knockoff','mvtnorm','hdi',
'foreach','doParallel')
lapply(pkgs, library, character.only = TRUE)
# === PARAMETER GRID ===
param_grid <- expand.grid(
s = 1:50,
i = seq(from = 7, to = 13, by = 1)
)
# === SET UP PARALLEL BACKEND ===
cl <- makeCluster(20)
# export working dir so workers can source
clusterExport(cl, 'mywd')
# have each worker source & load libraries
clusterEvalQ(cl, {
setwd(mywd)
source(file.path(mywd, 'Functions', 'TriangleBoosterTrainMS.R'))
source(file.path(mywd, 'Functions', 'ApplyGBMKnockoff.R'))
source(file.path(mywd, 'Functions', 'HelperFunctions.R'))
source(file.path(mywd, 'Functions Dai', 'knockoff.R'))
source(file.path(mywd, 'Functions Dai', 'analysis.R'))
source(file.path(mywd, 'Functions Dai', 'MBHq.R'))
source(file.path(mywd, 'Functions Dai', 'DS.R'))
source(file.path(mywd, 'Functions Dai', 'fdp_power.R'))
lapply(c('xgboost','gbm','MASS','glmnet','knockoff','mvtnorm','hdi'),
library, character.only = TRUE)
})
registerDoParallel(cl)
# === RUN IN PARALLEL AND WRITE OUT ===
results_list <- foreach(
k = seq_len(nrow(param_grid)),
.packages = pkgs,
.combine  = rbind
) %dopar% {
s_val <- param_grid$s[k]
i_val <- param_grid$i[k]
# compute chunk of results
chunk <- Compare_SignalStrength(i = i_val, s = s_val)
# write out this chunk immediately
fname <- sprintf("Results_s%02d_i%02d.csv", s_val, i_val)
write.csv(chunk, file = paste0(mywd,"/Temp2/",fname), row.names = FALSE)
# return for final binding
chunk
}
# === CLEANUP AND FINAL SAVE ===
stopCluster(cl)
warnings()
# combine all and save full dataset
Results <- results_list
write.csv(Results, file = "ResultsNonlinearScenario2.csv", row.names = FALSE)
