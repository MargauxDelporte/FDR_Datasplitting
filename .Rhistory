n <- dim(X)[1]; p <- dim(X)[2]
inclusion_rate <- matrix(0, nrow = num_split, ncol = p)
fdp <- rep(0, num_split)
power <- rep(0, num_split)
num_select <- rep(0, num_split)
data<-data.frame(cbind(y,X))
names(data)=c('y',paste0('X',1:p))
train_index<-sample(x = c(1:n), size = amountTrain * n, replace = F)
dataTrain<-data[train_index,]
colnames(dataTrain)<-c('y',paste0('X',1:p))
colnames(data)<-c('y',paste0('X',1:p))
lm<-xgboost(data = as.matrix(X[train_index,]), label =y[train_index],nrounds=1000,lambda=1,eta=0.005,verbose=F,booster='gbtree')
remaining_percent=1-amountTrain
remaining_index<-c(setdiff(c(1:n),train_index),sample(train_index,size=remaining_percent*n))
sample_index1 <- remaining_index
predictLM1<-predict(lm,newdata=as.matrix(data[remaining_index,]))
predictLM1<-predict(lm,newdata=as.matrix(X[remaining_index,]))
predictLM1
R2orig1<-1-sum((y[remaining_index]-predictLM1)^2)/sum((y[remaining_index]-mean(y[remaining_index]))^2)
R2orig1
Rnew1<-sapply(1:ncol(X),function(j) permR2BoostNoSplit(data[remaining_index,],j,lm))
Rnew1
Diff1=R2orig1-Rnew1
Diff1
sd_X1 <- apply(X[sample_index1, ], 2, sd)
beta1=sign(Diff1)*sqrt(abs(Diff1))*sd(y)/sd_X1
mirror<-beta1
selected_index<-SelectFeatures(mirror,abs(mirror),0.1)
selected_index
### number of selected variables j=1
if(length(selected_index)!=0){
num_select[iter] <- length(selected_index)
inclusion_rate[iter, selected_index] <- 1/num_select[iter]
### calculate fdp and power
result <- CalculateFDP_Power(selected_index, signal_index)
fdp[iter] <- result$fdp
power[iter] <- result$power
}
for(iter in 1:num_split){
train_index<-sample(x = c(1:n), size = amountTrain * n, replace = F)
dataTrain<-data[train_index,]
colnames(dataTrain)<-c('y',paste0('X',1:p))
colnames(data)<-c('y',paste0('X',1:p))
lm<-xgboost(data = as.matrix(X[train_index,]), label =y[train_index],nrounds=1000,lambda=1,eta=0.005,verbose=F,booster='gbtree')
remaining_percent=1-amountTrain
remaining_index<-c(setdiff(c(1:n),train_index),sample(train_index,size=remaining_percent*n))
sample_index1 <- remaining_index
predictLM1<-predict(lm,newdata=as.matrix(X[remaining_index,]))
R2orig1<-1-sum((y[remaining_index]-predictLM1)^2)/sum((y[remaining_index]-mean(y[remaining_index]))^2)
Rnew1<-sapply(1:ncol(X),function(j) permR2BoostNoSplit(data[remaining_index,],j,lm))
Diff1=R2orig1-Rnew1
sd_X1 <- apply(X[sample_index1, ], 2, sd)
beta1=sign(Diff1)*sqrt(abs(Diff1))*sd(y)/sd_X1
mirror<-beta1
selected_index<-SelectFeatures(mirror,abs(mirror),0.1)
### number of selected variables j=1
if(length(selected_index)!=0){
num_select[iter] <- length(selected_index)
inclusion_rate[iter, selected_index] <- 1/num_select[iter]
### calculate fdp and power
result <- CalculateFDP_Power(selected_index, signal_index)
fdp[iter] <- result$fdp
power[iter] <- result$power
}
}
### single data-splitting (DS) result
DS_fdp <- fdp[1]
DS_power <- power[1]
###my own methods:
g2=ApplyBoostNoSplit(X=as.data.frame(X), y, q=0.1,num_split=10, signal_index=signal_index, amountTrain=0.5, myseed = 1)
#source('TriangleLinRegTrainMS.R')
source(paste0(mywd,'/NoSplit/NoSplitBooster.R'))
###my own methods:
g2=ApplyBoostNoSplit(X=as.data.frame(X), y, q=0.1,num_split=10, signal_index=signal_index, amountTrain=0.5, myseed = 1)
#mydata=data[remaining_index,]
permR2BoostNoSplit<-function(mydata,j,model){
dataPerm<-mydata[,-1]
dataPerm[,j]<-sample(mydata[,j+1],replace=FALSE)
names(dataPerm)=c(paste0('X',1:p))
predictLM<-predict(model,newdata=as.matrix(dataPerm))
rsquared=1-sum((mydata$y-predictLM)^2)/sum((mydata$y-mean(mydata$y))^2)
return(rsquared)
}
ApplyBoostNoSplit<-function(X, y, q,amountTrain=0.5,myseed,num_split=1,signal_index=signal_index){
set.seed(myseed)
n <- dim(X)[1]; p <- dim(X)[2]
inclusion_rate <- matrix(0, nrow = num_split, ncol = p)
fdp <- rep(0, num_split)
power <- rep(0, num_split)
num_select <- rep(0, num_split)
data<-data.frame(cbind(y,X))
names(data)=c('y',paste0('X',1:p))
for(iter in 1:num_split){
train_index<-sample(x = c(1:n), size = amountTrain * n, replace = F)
dataTrain<-data[train_index,]
colnames(dataTrain)<-c('y',paste0('X',1:p))
colnames(data)<-c('y',paste0('X',1:p))
names(X)=c(paste0('X',1:p))
lm<-xgboost(data = as.matrix(X[train_index,]), label =y[train_index],nrounds=1000,lambda=1,eta=0.005,verbose=F,booster='gbtree')
remaining_percent=1-amountTrain
remaining_index<-c(setdiff(c(1:n),train_index),sample(train_index,size=remaining_percent*n))
sample_index1 <- remaining_index
predictLM1<-predict(lm,newdata=as.matrix(X[remaining_index,]))
R2orig1<-1-sum((y[remaining_index]-predictLM1)^2)/sum((y[remaining_index]-mean(y[remaining_index]))^2)
Rnew1<-sapply(1:ncol(X),function(j) permR2BoostNoSplit(data[remaining_index,],j,lm))
Diff1=R2orig1-Rnew1
sd_X1 <- apply(X[sample_index1, ], 2, sd)
beta1=sign(Diff1)*sqrt(abs(Diff1))*sd(y)/sd_X1
mirror<-beta1
selected_index<-SelectFeatures(mirror,abs(mirror),0.1)
### number of selected variables j=1
if(length(selected_index)!=0){
num_select[iter] <- length(selected_index)
inclusion_rate[iter, selected_index] <- 1/num_select[iter]
### calculate fdp and power
result <- CalculateFDP_Power(selected_index, signal_index)
fdp[iter] <- result$fdp
power[iter] <- result$power
}
}
### single data-splitting (DS) result
DS_fdp <- fdp[1]
DS_power <- power[1]
### multiple data-splitting (MDS) result
inclusion_rate <- apply(inclusion_rate, 2, mean)
### rank the features by the empirical inclusion rate
feature_rank <- order(inclusion_rate)
feature_rank <- setdiff(feature_rank, which(inclusion_rate == 0))
if(length(feature_rank)!=0){
null_feature <- numeric()
### backtracking
for(feature_index in 1:length(feature_rank)){
if(sum(inclusion_rate[feature_rank[1:feature_index]]) > q){
break
}else{
null_feature <- c(null_feature, feature_rank[feature_index])
}
}
selected_index <- setdiff(feature_rank, null_feature)
### calculate fdp and power
result <- CalculateFDP_Power(selected_index, signal_index)
MDS_fdp <- result$fdp
MDS_power <- result$power
}
else{
MDS_fdp <- 0
MDS_power <- 0
}
return(list(DS_fdp = DS_fdp, DS_power = DS_power, MDS_fdp = MDS_fdp, MDS_power = MDS_power))
}
###my own methods:
g2=ApplyBoostNoSplit(X=as.data.frame(X), y, q=0.1,num_split=10, signal_index=signal_index, amountTrain=0.5, myseed = 1)
### algorithmic settings
num_split <- 10
n <-80
p <- 20
p0 <- 2
q <- 0.1
#set.seed(124)(123) i=5
set.seed(456)
signal_index <- sample(c(1:p), size = p0, replace = F)
ResultsDataFrame=data.frame()
delta <- i
X <- mvrnorm(n, mu = rep(0, p), Sigma = diag(p))
### randomly generate the true beta i=4
beta_star <- rep(0, p)
beta_star[signal_index] <- 25#rnorm(p0, mean = 0, sd = delta*sqrt(log(p)/n))
### generate y
y <- X^2%*%beta_star + rnorm(n, mean = 0, sd = 1)
###my own methods:
g2=ApplyBoostNoSplit(X=as.data.frame(X), y, q=0.1,num_split=10, signal_index=signal_index, amountTrain=0.5, myseed = 1)
###my own methods:
g2=ApplyBoostNoSplit(X=as.data.frame(X), y, q=0.1,num_split=1, signal_index=signal_index, amountTrain=0.5, myseed = 1)
g2
y
n <-80
p <- 20
p0 <- 2
beta_star
beta_star[signal_index] <- 25#rnorm(p0, mean = 0, sd = delta*sqrt(log(p)/n))
### generate y
y <- X^2%*%beta_star + rnorm(n, mean = 0, sd = 1)
###my own methods:
g2=ApplyBoostNoSplit(X=as.data.frame(X), y, q=0.1,num_split=1, signal_index=signal_index, amountTrain=0.5, myseed = 1)
g2
n <-800
p <- 20
###my own methods:
g2=ApplyBoostNoSplit(X=as.data.frame(X), y, q=0.1,num_split=1, signal_index=signal_index, amountTrain=0.5, myseed = 1)
ResultsDataFrame=data.frame()
delta <- i
X <- mvrnorm(n, mu = rep(0, p), Sigma = diag(p))
### randomly generate the true beta i=4
beta_star <- rep(0, p)
beta_star[signal_index] <- 25#rnorm(p0, mean = 0, sd = delta*sqrt(log(p)/n))
### generate y
y <- X^2%*%beta_star + rnorm(n, mean = 0, sd = 1)
###my own methods:
g2=ApplyBoostNoSplit(X=as.data.frame(X), y, q=0.1,num_split=1, signal_index=signal_index, amountTrain=0.5, myseed = 1)
g2
### algorithmic settings
num_split <- 10
n <-800
p <- 50
p0 <- 10
q <- 0.1
ResultsDataFrame=data.frame()
delta <- i
X <- mvrnorm(n, mu = rep(0, p), Sigma = diag(p))
### randomly generate the true beta i=4
beta_star <- rep(0, p)
beta_star[signal_index] <- 25#rnorm(p0, mean = 0, sd = delta*sqrt(log(p)/n))
### generate y
y <- X^2%*%beta_star + rnorm(n, mean = 0, sd = 1)
###my own methods:
g2=ApplyBoostNoSplit(X=as.data.frame(X), y, q=0.1,num_split=1, signal_index=signal_index, amountTrain=0.5, myseed = 1)
ResultsDataFrame=c('2 split DS',i, as.numeric(g2$DS_fdp),as.numeric(g2$DS_power))
ResultsDataFrame=rbind(ResultsDataFrame,c('2 splitMS',i, as.numeric(g2$MDS_fdp),as.numeric(g2$MDS_power)))
g2
###my own methods:
g2=ApplyBoostNoSplit(X=as.data.frame(X), y, q=0.1,num_split=5, signal_index=signal_index, amountTrain=0.5, myseed = 1)
### randomly generate the true beta i=4
beta_star <- rep(0, p)
beta_star[signal_index] <- 0#25#rnorm(p0, mean = 0, sd = delta*sqrt(log(p)/n))
### generate y
y <- X^2%*%beta_star + rnorm(n, mean = 0, sd = 1)
y
beta_star
n <- dim(X)[1]; p <- dim(X)[2]
inclusion_rate <- matrix(0, nrow = num_split, ncol = p)
fdp <- rep(0, num_split)
power <- rep(0, num_split)
num_select <- rep(0, num_split)
data<-data.frame(cbind(y,X))
names(data)=c('y',paste0('X',1:p))
for(iter in 1:num_split){
train_index<-sample(x = c(1:n), size = amountTrain * n, replace = F)
dataTrain<-data[train_index,]
colnames(dataTrain)<-c('y',paste0('X',1:p))
colnames(data)<-c('y',paste0('X',1:p))
names(X)=c(paste0('X',1:p))
lm<-xgboost(data = as.matrix(X[train_index,]), label =y[train_index],nrounds=1000,lambda=1,eta=0.005,verbose=F,booster='gbtree')
remaining_percent=1-amountTrain
remaining_index<-c(setdiff(c(1:n),train_index),sample(train_index,size=remaining_percent*n))
sample_index1 <- remaining_index
predictLM1<-predict(lm,newdata=as.matrix(X[remaining_index,]))
R2orig1<-1-sum((y[remaining_index]-predictLM1)^2)/sum((y[remaining_index]-mean(y[remaining_index]))^2)
Rnew1<-sapply(1:ncol(X),function(j) permR2BoostNoSplit(data[remaining_index,],j,lm))
Diff1=R2orig1-Rnew1
sd_X1 <- apply(X[sample_index1, ], 2, sd)
beta1=sign(Diff1)*sqrt(abs(Diff1))*sd(y)/sd_X1
mirror<-beta1
selected_index<-SelectFeatures(mirror,abs(mirror),0.1)
### number of selected variables j=1
if(length(selected_index)!=0){
num_select[iter] <- length(selected_index)
inclusion_rate[iter, selected_index] <- 1/num_select[iter]
### calculate fdp and power
result <- CalculateFDP_Power(selected_index, signal_index)
fdp[iter] <- result$fdp
power[iter] <- result$power
}
}
train_index<-sample(x = c(1:n), size = amountTrain * n, replace = F)
dataTrain<-data[train_index,]
colnames(dataTrain)<-c('y',paste0('X',1:p))
colnames(data)<-c('y',paste0('X',1:p))
names(X)=c(paste0('X',1:p))
lm<-xgboost(data = as.matrix(X[train_index,]), label =y[train_index],nrounds=1000,lambda=1,eta=0.005,verbose=F,booster='gbtree')
remaining_percent=1-amountTrain
remaining_index<-c(setdiff(c(1:n),train_index),sample(train_index,size=remaining_percent*n))
sample_index1 <- remaining_index
predictLM1<-predict(lm,newdata=as.matrix(X[remaining_index,]))
R2orig1<-1-sum((y[remaining_index]-predictLM1)^2)/sum((y[remaining_index]-mean(y[remaining_index]))^2)
Rnew1<-sapply(1:ncol(X),function(j) permR2BoostNoSplit(data[remaining_index,],j,lm))
Diff1=R2orig1-Rnew1
sd_X1 <- apply(X[sample_index1, ], 2, sd)
beta1=sign(Diff1)*sqrt(abs(Diff1))*sd(y)/sd_X1
mirror<-beta1
mirror2=c(mirror2,mirror)
mirror2=c()
#mirror2=c()
mirror2=c(mirror2,mirror)
mirror2
mirror2=c(mirror2,mirror)
train_index<-sample(x = c(1:n), size = amountTrain * n, replace = F)
dataTrain<-data[train_index,]
colnames(dataTrain)<-c('y',paste0('X',1:p))
colnames(data)<-c('y',paste0('X',1:p))
names(X)=c(paste0('X',1:p))
lm<-xgboost(data = as.matrix(X[train_index,]), label =y[train_index],nrounds=1000,lambda=1,eta=0.005,verbose=F,booster='gbtree')
remaining_percent=1-amountTrain
remaining_index<-c(setdiff(c(1:n),train_index),sample(train_index,size=remaining_percent*n))
sample_index1 <- remaining_index
predictLM1<-predict(lm,newdata=as.matrix(X[remaining_index,]))
R2orig1<-1-sum((y[remaining_index]-predictLM1)^2)/sum((y[remaining_index]-mean(y[remaining_index]))^2)
Rnew1<-sapply(1:ncol(X),function(j) permR2BoostNoSplit(data[remaining_index,],j,lm))
Diff1=R2orig1-Rnew1
sd_X1 <- apply(X[sample_index1, ], 2, sd)
beta1=sign(Diff1)*sqrt(abs(Diff1))*sd(y)/sd_X1
mirror<-beta1
#mirror2=c()
mirror2=c(mirror2,mirror)
train_index<-sample(x = c(1:n), size = amountTrain * n, replace = F)
dataTrain<-data[train_index,]
colnames(dataTrain)<-c('y',paste0('X',1:p))
colnames(data)<-c('y',paste0('X',1:p))
names(X)=c(paste0('X',1:p))
lm<-xgboost(data = as.matrix(X[train_index,]), label =y[train_index],nrounds=1000,lambda=1,eta=0.005,verbose=F,booster='gbtree')
remaining_percent=1-amountTrain
remaining_index<-c(setdiff(c(1:n),train_index),sample(train_index,size=remaining_percent*n))
sample_index1 <- remaining_index
predictLM1<-predict(lm,newdata=as.matrix(X[remaining_index,]))
R2orig1<-1-sum((y[remaining_index]-predictLM1)^2)/sum((y[remaining_index]-mean(y[remaining_index]))^2)
Rnew1<-sapply(1:ncol(X),function(j) permR2BoostNoSplit(data[remaining_index,],j,lm))
Diff1=R2orig1-Rnew1
sd_X1 <- apply(X[sample_index1, ], 2, sd)
beta1=sign(Diff1)*sqrt(abs(Diff1))*sd(y)/sd_X1
mirror<-beta1
#mirror2=c()
mirror2=c(mirror2,mirror)
mirror2
y
mirror2
hist(mirror2)
n <-800
p <- 100
X <- mvrnorm(n, mu = rep(0, p), Sigma = diag(p))
### randomly generate the true beta i=4
beta_star <- rep(0, p)
beta_star[signal_index] <- 0#25#rnorm(p0, mean = 0, sd = delta*sqrt(log(p)/n))
### generate y
y <- X^2%*%beta_star + rnorm(n, mean = 0, sd = 1)
###my own methods:
g2=ApplyBoostNoSplit(X=as.data.frame(X), y, q=0.1,num_split=5, signal_index=signal_index, amountTrain=0.5, myseed = 1)
data<-data.frame(cbind(y,X))
train_index<-sample(x = c(1:n), size = amountTrain * n, replace = F)
dataTrain<-data[train_index,]
colnames(dataTrain)<-c('y',paste0('X',1:p))
colnames(data)<-c('y',paste0('X',1:p))
names(X)=c(paste0('X',1:p))
lm<-xgboost(data = as.matrix(X[train_index,]), label =y[train_index],nrounds=1000,lambda=1,eta=0.005,verbose=F,booster='gbtree')
remaining_percent=1-amountTrain
remaining_index<-c(setdiff(c(1:n),train_index),sample(train_index,size=remaining_percent*n))
sample_index1 <- remaining_index
predictLM1<-predict(lm,newdata=as.matrix(X[remaining_index,]))
R2orig1<-1-sum((y[remaining_index]-predictLM1)^2)/sum((y[remaining_index]-mean(y[remaining_index]))^2)
Rnew1<-sapply(1:ncol(X),function(j) permR2BoostNoSplit(data[remaining_index,],j,lm))
Diff1=R2orig1-Rnew1
sd_X1 <- apply(X[sample_index1, ], 2, sd)
beta1=sign(Diff1)*sqrt(abs(Diff1))*sd(y)/sd_X1
mirror<-beta1
#mirror2=c()
mirror2=c(mirror2,mirror)
hist(mirror2)
n <- dim(X)[1]; p <- dim(X)[2]
inclusion_rate <- matrix(0, nrow = num_split, ncol = p)
fdp <- rep(0, num_split)
power <- rep(0, num_split)
num_select <- rep(0, num_split)
data<-data.frame(cbind(y,X))
names(data)=c('y',paste0('X',1:p))
train_index<-sample(x = c(1:n), size = amountTrain * n, replace = F)
dataTrain<-data[train_index,]
colnames(dataTrain)<-c('y',paste0('X',1:p))
colnames(data)<-c('y',paste0('X',1:p))
names(X)=c(paste0('X',1:p))
library(randomForest)
install.packages("randomForest")
library(randomForest)
randomForest
lm<-randomForest(data = as.matrix(X[train_index,]), label =y[train_index])
lm<-randomForest(x = as.matrix(X[train_index,]), label =y[train_index])
lm
remaining_percent=1-amountTrain
remaining_index<-c(setdiff(c(1:n),train_index),sample(train_index,size=remaining_percent*n))
sample_index1 <- remaining_index
predictLM1<-predict(lm,newdata=as.matrix(X[remaining_index,]))
predictLM1
R2orig1<-1-sum((y[remaining_index]-predictLM1)^2)/sum((y[remaining_index]-mean(y[remaining_index]))^2)
R2orig1
Rnew1<-sapply(1:ncol(X),function(j) permR2BoostNoSplit(data[remaining_index,],j,lm))
Diff1=R2orig1-Rnew1
j=1
mydata=data[remaining_index,]
dataPerm<-mydata[,-1]
dataPerm[,j]<-sample(mydata[,j+1],replace=FALSE)
names(dataPerm)=c(paste0('X',1:p))
lm
model=lm
predictLM<-predict(model,x=as.matrix(dataPerm))
predictLM<-predict(model,newdata=as.matrix(dataPerm))
predictLM<-predict(model,newdata=as.matrix(dataPerm))
model
predict(lm,newdata=as.matrix(X[remaining_index,]))
predictLM1<-predict(lm,newdata=as.matrix(X[remaining_index,]))
predictLM1<-predict(lm, newdata = as.matrix(X[remaining_index, ]))
num_split <- 10
n <-800
p <- 100
p0 <- 10
q <- 0.1
i=25
delta <- i
X <- mvrnorm(n, mu = rep(0, p), Sigma = diag(p))
### randomly generate the true beta i=4
beta_star <- rep(0, p)
beta_star[signal_index] <- 0#25#rnorm(p0, mean = 0, sd = delta*sqrt(log(p)/n))
### generate y
y <- X^2%*%beta_star + rnorm(n, mean = 0, sd = 1)
###my own methods:
g2=ApplyBoostNoSplit(X=as.data.frame(X), y, q=0.1,num_split=5, signal_index=signal_index, amountTrain=0.5, myseed = 1)
### generate y
y <- X^2%*%beta_star + rnorm(n, mean = 0, sd = 1)
y
data<-data.frame(cbind(y,X))
train_index<-sample(x = c(1:n), size = amountTrain * n, replace = F)
dataTrain<-data[train_index,]
colnames(dataTrain)<-c('y',paste0('X',1:p))
data<-data.frame(cbind(y,X))
data
train_index<-sample(x = c(1:n), size = amountTrain * n, replace = F)
dataTrain<-data[train_index,]
amountTrain=0.5
dataTrain<-data[train_index,]
colnames(dataTrain)<-c('y',paste0('X',1:p))
dataTrain<-data[train_index,]
train_index<-sample(x = c(1:n), size = amountTrain * n, replace = F)
dataTrain<-data[train_index,]
colnames(dataTrain)<-c('y',paste0('X',1:p))
colnames(data)<-c('y',paste0('X',1:p))
names(X)=c(paste0('X',1:p))
rf<-randomForest(x = as.matrix(X[train_index,]), label =y[train_index])
remaining_percent=1-amountTrain
remaining_index<-c(setdiff(c(1:n),train_index),sample(train_index,size=remaining_percent*n))
sample_index1 <- remaining_index
predictLM1<-predict(rf, newdata = as.matrix(X[remaining_index, ]))
predictLM1<-predict(rf, newdata = as.matrix(X[remaining_index, ]))
rf<-randomForest(x = as.matrix(X[train_index, ]), y = y[train_index])
predictLM1<-predict(rf, newdata = as.matrix(X[remaining_index, ]))
predictLM1
R2orig1<-1-sum((y[remaining_index]-predictLM1)^2)/sum((y[remaining_index]-mean(y[remaining_index]))^2)
R2orig1
#mydata=data[remaining_index,]  j=1 model=lm
permR2RFNoSplit<-function(mydata,j,model){
dataPerm<-mydata[,-1]
dataPerm[,j]<-sample(mydata[,j+1],replace=FALSE)
names(dataPerm)=c(paste0('X',1:p))
predictLM<-predict(model, newdata = as.matrix(dataPerm))
rsquared=1-sum((mydata$y-predictLM)^2)/sum((mydata$y-mean(mydata$y))^2)
return(rsquared)
}
ApplyRFNoSplit
#mydata=data[remaining_index,]  j=1 model=lm
permR2RFNoSplit<-function(mydata,j,model){
dataPerm<-mydata[,-1]
dataPerm[,j]<-sample(mydata[,j+1],replace=FALSE)
names(dataPerm)=c(paste0('X',1:p))
predictLM<-predict(model, newdata = as.matrix(dataPerm))
rsquared=1-sum((mydata$y-predictLM)^2)/sum((mydata$y-mean(mydata$y))^2)
return(rsquared)
}
j=1
rf
model=rf
j=1
dataPerm<-mydata[,-1]
dataPerm[,j]<-sample(mydata[,j+1],replace=FALSE)
names(dataPerm)=c(paste0('X',1:p))
mydata=data[remaining_index,]
dataPerm<-mydata[,-1]
dataPerm[,j]<-sample(mydata[,j+1],replace=FALSE)
names(dataPerm)=c(paste0('X',1:p))
predictLM<-predict(model, newdata = as.matrix(dataPerm))
newdata = as.matrix(dataPerm)
as.matrix(dataPerm)
predictLM<-predict(model, newdata = as.matrix(dataPerm))
names(X)=c(paste0('X',1:p))
names(X)=c(paste0('X',1:p))
rf<-randomForest(x = as.matrix(X[train_index, ]), y = y[train_index])
remaining_percent=1-amountTrain
remaining_index<-c(setdiff(c(1:n),train_index),sample(train_index,size=remaining_percent*n))
sample_index1 <- remaining_index
predictLM1<-predict(rf, newdata = as.matrix(X[remaining_index, ]))
predictLM1
R2orig1<-1-sum((y[remaining_index]-predictLM1)^2)/sum((y[remaining_index]-mean(y[remaining_index]))^2)
R2orig1
dataPerm<-mydata[,-1]
dataPerm[,j]<-sample(mydata[,j+1],replace=FALSE)
names(dataPerm)=c(paste0('X',1:p))
predictLM<-predict(model, newdata = as.matrix(dataPerm))
as.matrix(dataPerm)
X[remaining_index, ]
names(dataPerm)=c()
predictLM<-predict(model, newdata = as.matrix(dataPerm))
rsquared=1-sum((mydata$y-predictLM)^2)/sum((mydata$y-mean(mydata$y))^2)
rsquared
#mydata=data[remaining_index,]  j=1 model=rf
permR2RFNoSplit<-function(mydata,j,model){
dataPerm<-mydata[,-1]
dataPerm[,j]<-sample(mydata[,j+1],replace=FALSE)
names(dataPerm)=c()
predictLM<-predict(model, newdata = as.matrix(dataPerm))
rsquared=1-sum((mydata$y-predictLM)^2)/sum((mydata$y-mean(mydata$y))^2)
return(rsquared)
}
Rnew1<-sapply(1:ncol(X),function(j) permR2RFNoSplit(data[remaining_index,],j,rf))
Diff1=R2orig1-Rnew1
sd_X1 <- apply(X[sample_index1, ], 2, sd)
beta1=sign(Diff1)*sqrt(abs(Diff1))*sd(y)/sd_X1
mirror<-beta1
mirror
