s=10
set.seed(s)
delta <- i
signal_index <- sample(c(1:p), size = p0, replace = F)
# simulate data
n1 <- floor(n/2); n2 <- n - n1
X1 <- matrix(rnorm(n1*p, mean= 2), n1, p)
X2 <- matrix(rnorm(n2*p, mean=-2), n2, p)
X  <- rbind(X1, X2)
beta_star <- numeric(p)
beta_star[signal_index] <- rnorm(p0, 0, delta*sqrt(log(p)/n))*10
y <- scale(X^2 %*% beta_star + rnorm(n))
# --- your fixed settings ------------------------
set.seed(23)
delta <- 10
signal_index <- sample(c(1:p), size = p0, replace = F)
# simulate data
n1 <- floor(n/2); n2 <- n - n1
X1 <- matrix(rnorm(n1*p, mean= 2), n1, p)
X2 <- matrix(rnorm(n2*p, mean=-2), n2, p)
X  <- rbind(X1, X2)
beta_star <- numeric(p)
beta_star[signal_index] <- rnorm(p0, 0, delta*sqrt(log(p)/n))*10
y <- scale(X^2 %*% beta_star + rnorm(n))
library(xgboost)
# --- your fixed settings ------------------------
set.seed(23)
delta <- 10
signal_index <- sample(c(1:p), size = p0, replace = F)
# simulate data
n1 <- floor(n/2); n2 <- n - n1
X1 <- matrix(rnorm(n1*p, mean= 2), n1, p)
X2 <- matrix(rnorm(n2*p, mean=-2), n2, p)
X  <- rbind(X1, X2)
beta_star <- numeric(p)
beta_star[signal_index] <- rnorm(p0, 0, delta*sqrt(log(p)/n))*10
set.seed(23)
num_split <- 50
n <-750
p <- 250
p0 <- 20
q <- 0.1
delta <- 10
signal_index <- sample(c(1:p), size = p0, replace = F)
# simulate data
n1 <- floor(n/2); n2 <- n - n1
X1 <- matrix(rnorm(n1*p, mean= 2), n1, p)
X2 <- matrix(rnorm(n2*p, mean=-2), n2, p)
X  <- rbind(X1, X2)
beta_star <- numeric(p)
beta_star[signal_index] <- rnorm(p0, 0, delta*sqrt(log(p)/n))*10
y <- scale(X^2 %*% beta_star + rnorm(n))
nrounds    <- 500
num_split  <- 1
amountTrain<- 0.333
# assume X (n×p) and y (length n) already exist
data <- data.frame(y = y, X)
# --- define a grid of candidate params -----------
param_grid <- expand.grid(
eta               = c(0.01, 0.05, 0.1),
max_depth         = c(3, 4, 6),
subsample         = c(0.6, 0.8, 1.0),
colsample_bytree  = c(0.6, 0.8, 1.0),
lambda            = c(0, 1, 5),
alpha             = c(0, 1),
booster           = "gbtree",
stringsAsFactors  = FALSE
)
# prepare storage
param_grid$mean_R2 <- NA_real_
# helper to compute R2
calc_r2 <- function(obs, pred) {
1 - sum((obs - pred)^2) / sum((obs - mean(obs))^2)
}
# --- grid search over params ----------------------
for(j in 1:nrow(param_grid)) {
pars   <- as.list(param_grid[j, c("eta","max_depth","subsample",
"colsample_bytree","lambda","alpha",
"booster")])
R2_vals <- numeric(num_split)
for(iter in seq_len(num_split)) {
# split train/test
train_idx <- sample(seq_len(nrow(data)), size = amountTrain * nrow(data))
test_idx  <- setdiff(seq_len(nrow(data)), train_idx)
dtrain <- xgb.DMatrix(
data  = as.matrix(data[train_idx, -1]),
label = data$y[train_idx]
)
dtest  <- xgb.DMatrix(
data  = as.matrix(data[test_idx,  -1]),
label = data$y[test_idx]
)
# fit model
bst <- xgb.train(
params   = pars,
data     = dtrain,
nrounds  = nrounds,
verbose  = 0
)
# predict & compute R2
pred         <- predict(bst, dtest)
R2_vals[iter] <- calc_r2(data$y[test_idx], pred)
}
# store average R2
param_grid$mean_R2[j] <- mean(R2_vals)
print(j)
}
# --- pick best -------------------------------
best_row   <- which.max(param_grid$mean_R2)
best_param <- param_grid[best_row, ]
print(best_param)
### algorithmic settings
num_split <- 50
n <-750
p <- 250
p0 <- 20
q <- 0.1
#set.seed(124)(123) i=5
set.seed(456)
signal_index <- sample(c(1:p), size = p0, replace = F)
###choose the parameters
params =list(
objective = "reg:squarederror",
eta       = 0.05,
max_depth = 3,
subsample = 0.8,
colsample_bytree = 0.8,
lambda    = 0,
alpha     = 0
)
### High dimension linear model
rm(list = ls())
mywd='C:/Users/mde4023/Downloads/FDR_Datasplitting'
#mywd='C:/Users/mde4023/Documents/GitHub/FDR_Datasplitting'
setwd(mywd)
source(paste0(mywd,'/Functions/HelperFunctions.R'))
source(paste0(mywd,'/Functions/TriangleBoosterTrainMS.R'))
source(paste0(mywd,'/Functions/ApplyGBMKnockoff.R'))
#source(paste0(mywd,'/Functions/TriangleGBMTrainMS.R'))
source(paste0(mywd,'/Functions Dai/knockoff.R'))
source(paste0(mywd,'/Functions Dai/analysis.R'))
source(paste0(mywd,'/Functions Dai/MBHq.R'))
source(paste0(mywd,'/Functions Dai/DS.R'))
source(paste0(mywd,'/Functions Dai/fdp_power.R'))
#devtools::install_github("Jeremy690/DSfdr/DSfdr",force = TRUE)
library(xgboost)
library(gbm)
library(ranger)
library(MASS)
library(neuralnet)
library(glmnet)
library(knockoff)
library(mvtnorm)
library(hdi)
### algorithmic settings
num_split <- 50
n <-750
p <- 250
p0 <- 20
q <- 0.1
#set.seed(124)(123) i=5
set.seed(456)
signal_index <- sample(c(1:p), size = p0, replace = F)
###choose the parameters
params =list(
objective = "reg:squarederror",
eta       = 0.05,
max_depth = 3,
subsample = 0.8,
colsample_bytree = 0.8,
lambda    = 0,
alpha     = 0
)
#######set up the method for the comparison############# i=10 s=10 num_split=1
Compare_SignalStrength <- function(i, s) {
set.seed(s)
delta <- i
signal_index <- sample(c(1:p), size = p0, replace = F)
# simulate data
n1 <- floor(n/2); n2 <- n - n1
X1 <- matrix(rnorm(n1*p, mean= 2), n1, p)
X2 <- matrix(rnorm(n2*p, mean=-2), n2, p)
X  <- rbind(X1, X2)
beta_star <- numeric(p)
beta_star[signal_index] <- rnorm(p0, 0, delta*sqrt(log(p)/n))*10
y <- scale(X^2 %*% beta_star + rnorm(n))
# run your custom methods
g1 <- ApplyTriangleBoostTrain( X = X, y = y, q = q, num_split = num_split,param=params,
signal_index = signal_index, myseed = 1)
# FDR methods
#DS_result      <- DS(          X = X, y = y, q = q, num_split = num_split)
#knockoff_result<- ApplyGBMKnockoff(    X = X, y = y, q = q,param=params)
#BH_result      <- MBHq(        X = X, y = y, q = q, num_split = num_split)
# init empty results df
ResultsDataFrame <- data.frame(
Method = character(),
Delta  = numeric(),
FDP    = numeric(),
Power  = numeric(),
stringsAsFactors = FALSE
)
# bind all rows
ResultsDataFrame <- rbind(
ResultsDataFrame,
data.frame(Method = "Boost DS",                Delta = i, FDP = g1$DS_fdp,    Power = g1$DS_power),
data.frame(Method = "Boost MS",                Delta = i, FDP = g1$MDS_fdp,   Power = g1$MDS_power)#,
#data.frame(Method = "DataSplitting",           Delta = i, FDP = DS_result$DS_fdp,  Power = DS_result$DS_power),
# data.frame(Method = "MultipleDataSplitting",   Delta = i, FDP = DS_result$MDS_fdp, Power = DS_result$MDS_power),
# data.frame(Method = "Knockoff",                Delta = i, FDP = knockoff_result$fdp, Power = knockoff_result$power),
# data.frame(Method = "Benjamini–Hochberg (BH)", Delta = i, FDP = BH_result$fdp,     Power = BH_result$power)
)
return(ResultsDataFrame)
}
Compare_SignalStrength(11,5)
Compare_SignalStrength(10,8)
library(dplyr)
library(tableone)
library(reticulate)
library(smoothHR)      # For the WHAS dataset
library(survival)   # For survival analysis functions
library(ggplot2)    # For plotting
library(ggfortify)  # For autoplotting survival curves
library(correctedC)
library(smoothHR)      # For the WHAS dataset
library(survival)   # For survival analysis functions
library(ggplot2)    # For plotting
library(ggfortify)  # For autoplotting survival curves
library(correctedC)
# Install packages if they are not already installed
install.packages("smoothHR")
install.packages("smoothHR")
#use_condaenv("/Users/yushushi/opt/anaconda3/envs/tensorflow", required = TRUE)
use_condaenv("C:/Users/mde4023/AppData/Local/anaconda3/envs/tensorflow", required = TRUE)
#use_condaenv("/Users/yushushi/opt/anaconda3/envs/tensorflow", required = TRUE)
use_condaenv("C:/Users/mde4023/AppData/Local/anaconda3/envs/tensorflow", required = TRUE)
dnnModel1 <-deepsurv(data=trainData,
time_variable = "lenfol",
status_variable = "fstat",
frac=0.5,
activation="relu",
num_nodes=c(64L,128L,64L),
dropout=0.2,
early_stopping=TRUE,
epochs=1000L,
patience=50L,
batch_norm = TRUE,
batch_size=250L,
shuffle=TRUE)
deepsurv
# Install packages if they are not already installed
install.packages("smoothHR")
# Load the packages
library(smoothHR)      # For the WHAS dataset
library(survival)   # For survival analysis functions
library(ggplot2)    # For plotting
library(ggfortify)  # For autoplotting survival curves
library(correctedC)
library(devtools)
install_github("YushuShi/correctedC")
install_github("YushuShi/survivalContour")
library(smoothHR)      # For the WHAS dataset
library(survival)   # For survival analysis functions
library(ggplot2)    # For plotting
library(ggfortify)  # For autoplotting survival curves
library(devtools)
library(correctedC)
library(survminer)
library(dplyr)
library(tableone)
library(reticulate)
library(survivalmodels)
library(survival)
library(devtools)
library(survivalContour)
library(randomForestSRC)
library(fastDummies)
#use_condaenv("/Users/yushushi/opt/anaconda3/envs/tensorflow", required = TRUE)
use_condaenv("C:/Users/mde4023/AppData/Local/anaconda3/envs/tensorflow", required = TRUE)
use_condaenv("C:/Users/mde4023/AppData/Local/anaconda3/envs/r-reticulate", required = TRUE)
use_condaenv("C:/Users/mde4023/AppData/Local/anaconda3/envs/r-tf", required = TRUE)
pycox <- reticulate::import("pycox")
#use_condaenv("/Users/yushushi/opt/anaconda3/envs/tensorflow", required = TRUE)
use_condaenv("C:/Users/mde4023/AppData/Local/anaconda3/envs/r-tf", required = TRUE)
# Load the WHAS dataset
data(whas500)
# Check the structure of the dataset
str(whas500)
View(whas500)
ntest<-floor(nrow(whas500)/10)
trainIndex <- sample(1:nrow(whas500), nrow(whas500)-ntest)
testIndex <- setdiff(1:nrow(whas500), trainIndex)
trainData <- whas500[trainIndex,]
testData <- whas500[testIndex,]
names(whas500)
linear_fit <- coxph(
Surv(wa$lenfol, wa$fstat)~age+gender+hr+sysbp+diasbp+bmi+cvd+afb+sho+chf+av3+miord+mitype,
data=wa)
linear_fit
linear_fit <- coxph(
Surv(wa$lenfol, wa$fstat)~age+gender+hr+sysbp+diasbp+bmi+cvd+afb+sho+chf+av3+miord+mitype,
data=whas500)
linear_fit <- coxph(
Surv(whas500$lenfol, whas500$fstat)~age+gender+hr+sysbp+diasbp+bmi+cvd+afb+sho+chf+av3+miord+mitype,
data=whas500)
linear_fit
dnnModel1 <-deepsurv(data=trainData,
time_variable = "lenfol",
status_variable = "fstat",
frac=0.5,
activation="relu",
num_nodes=c(64L,128L,64L),
dropout=0.2,
early_stopping=TRUE,
epochs=1000L,
patience=50L,
batch_norm = TRUE,
batch_size=250L,
shuffle=TRUE)
pycox <- reticulate::import("pycox")
pycox <- reticulate::import("pycox")
library(reticulate)
library(reticulate)
pycox <- reticulate::import("pycox")
reticulate::conda_binary()
library(reticulate)
py_install(packages = c("pycox", "torchtuples"), envname = "r-tf", pip = TRUE)
#use_condaenv("/Users/yushushi/opt/anaconda3/envs/tensorflow", required = TRUE)
use_condaenv("C:/Users/mde4023/AppData/Local/anaconda3/envs/r-tf", required = TRUE)
pycox <- reticulate::import("pycox")
torchtuples <- reticulate::import("torchtuples")
#py_install(packages = c("pycox", "torchtuples"), envname = "r-tf", pip = TRUE)
py_install(c("pycox", "torchtuples"), pip = TRUE)
#py_install(packages = c("pycox", "torchtuples"), envname = "r-tf", pip = TRUE)
py_install(c("pycox"), pip = TRUE)
conda_create("r-pycox", packages = "python=3.9")
use_condaenv("r-pycox", required = TRUE)
py_install(c("pycox", "torchtuples"))
conda_create("r-pycox2", packages = "python=3.13")
use_condaenv("r-pycox2", required = TRUE)
py_install(c("pycox", "torchtuples"))
use_condaenv("C:\Users\mde4023\AppData\Local\anaconda3\envs\r-tf", required = TRUE)
use_condaenv("C:/Users/mde4023/AppData/Local/anaconda3/envs/r-tf", required = TRUE)
dnnModel1 <-deepsurv(data=trainData,
time_variable = "lenfol",
status_variable = "fstat",
frac=0.5,
activation="relu",
num_nodes=c(64L,128L,64L),
dropout=0.2,
early_stopping=TRUE,
epochs=1000L,
patience=50L,
batch_norm = TRUE,
batch_size=250L,
shuffle=TRUE)
library(reticulate)
# Make sure you’re pointing to the environment
use_condaenv("C:/Users/mde4023/AppData/Local/anaconda3/envs/r-tf", required = TRUE)
# Reinstall core Python and system DLLs
conda_install(envname = "r-tf", packages = "python=3.9", channel = "conda-forge", pip = FALSE)
# Reinstall pycox, torchtuples (use pip, not conda)
py_install(c("pycox", "torchtuples"))
### High dimension linear model
rm(list = ls())
mywd='C:/Users/mde4023/Downloads/FDR_Datasplitting'
#mywd='C:/Users/mde4023/Documents/GitHub/FDR_Datasplitting'
setwd(mywd)
source(paste0(mywd,'/Functions/HelperFunctions.R'))
source(paste0(mywd,'/Functions/TriangleBoosterTrainMS.R'))
source(paste0(mywd,'/Functions/ApplyGBMKnockoff.R'))
#source(paste0(mywd,'/Functions/TriangleGBMTrainMS.R'))
source(paste0(mywd,'/Functions Dai/knockoff.R'))
source(paste0(mywd,'/Functions Dai/analysis.R'))
source(paste0(mywd,'/Functions Dai/MBHq.R'))
source(paste0(mywd,'/Functions Dai/DS.R'))
source(paste0(mywd,'/Functions Dai/fdp_power.R'))
#devtools::install_github("Jeremy690/DSfdr/DSfdr",force = TRUE)
library(xgboost)
library(gbm)
library(ranger)
library(MASS)
library(neuralnet)
library(glmnet)
library(knockoff)
library(mvtnorm)
library(hdi)
### algorithmic settings
num_split <- 50
n <-750
p <- 250
p0 <- 20
q <- 0.1
#set.seed(124)(123) i=5
set.seed(456)
signal_index <- sample(c(1:p), size = p0, replace = F)
###choose the parameters
params =list(
objective = "reg:squarederror",
eta       = 0.05,
max_depth = 3,
subsample = 0.8,
colsample_bytree = 0.8,
lambda    = 0,
alpha     = 0
)
#######set up the method for the comparison############# i=10 s=10 num_split=1
Compare_SignalStrength <- function(i, s) {
set.seed(s)
delta <- i
signal_index <- sample(c(1:p), size = p0, replace = F)
# simulate data
n1 <- floor(n/2); n2 <- n - n1
X1 <- matrix(rnorm(n1*p, mean= 2), n1, p)
X2 <- matrix(rnorm(n2*p, mean=-2), n2, p)
X  <- rbind(X1, X2)
beta_star <- numeric(p)
beta_star[signal_index] <- rnorm(p0, 0, delta*sqrt(log(p)/n))*10
y <- scale(X^2 %*% beta_star + rnorm(n))
# run your custom methods
g1 <- ApplyTriangleBoostTrain( X = X, y = y, q = q, num_split = num_split,param=params,
signal_index = signal_index, myseed = 1)
# FDR methods
#DS_result      <- DS(          X = X, y = y, q = q, num_split = num_split)
#knockoff_result<- ApplyGBMKnockoff(    X = X, y = y, q = q,param=params)
#BH_result      <- MBHq(        X = X, y = y, q = q, num_split = num_split)
# init empty results df
ResultsDataFrame <- data.frame(
Method = character(),
Delta  = numeric(),
FDP    = numeric(),
Power  = numeric(),
stringsAsFactors = FALSE
)
# bind all rows
ResultsDataFrame <- rbind(
ResultsDataFrame,
data.frame(Method = "Boost DS",                Delta = i, FDP = g1$DS_fdp,    Power = g1$DS_power),
data.frame(Method = "Boost MS",                Delta = i, FDP = g1$MDS_fdp,   Power = g1$MDS_power)#,
#data.frame(Method = "DataSplitting",           Delta = i, FDP = DS_result$DS_fdp,  Power = DS_result$DS_power),
# data.frame(Method = "MultipleDataSplitting",   Delta = i, FDP = DS_result$MDS_fdp, Power = DS_result$MDS_power),
# data.frame(Method = "Knockoff",                Delta = i, FDP = knockoff_result$fdp, Power = knockoff_result$power),
# data.frame(Method = "Benjamini–Hochberg (BH)", Delta = i, FDP = BH_result$fdp,     Power = BH_result$power)
)
return(ResultsDataFrame)
}
#Compare_SignalStrength(10,8)
#######run the code#############
#Results=data.frame()
#for(s in 1:25){
#  for(i in seq(from=5,to=13,by=1)){
#  Results=rbind(Results,Compare_SignalStrength(i,s))
#  print(s)
#  }
#  print(Results)
#  }
library(parallel)
mywd='C:/Users/mde4023/Downloads/FDR_Datasplitting'
setwd(mywd)
# Source helper and method files
source(file.path(mywd, 'Functions', 'TriangleBoosterTrainMS.R'))
source(file.path(mywd, 'Functions', 'HelperFunctions.R'))
source(file.path(mywd, 'Functions', 'ApplyGBMKnockoff.R'))
# Dai’s routines
#source(file.path(mywd, 'Functions Dai', 'knockoff.R'))
source(file.path(mywd, 'Functions Dai', 'analysis.R'))
source(file.path(mywd, 'Functions Dai', 'MBHq.R'))
source(file.path(mywd, 'Functions Dai', 'DS.R'))
source(file.path(mywd, 'Functions Dai', 'fdp_power.R'))
# Load required packages
pkgs <- c('xgboost','gbm','ranger','MASS','glmnet','knockoff','mvtnorm','hdi',
'foreach','doParallel')
lapply(pkgs, library, character.only = TRUE)
# === PARAMETER GRID ===
param_grid <- expand.grid(
s = 1:50,
i = seq(from = 7, to = 13, by = 1)
)
# === SET UP PARALLEL BACKEND ===
cl <- makeCluster(20)
# export working dir so workers can source
clusterExport(cl, 'mywd')
# have each worker source & load libraries
clusterEvalQ(cl, {
setwd(mywd)
source(file.path(mywd, 'Functions', 'TriangleBoosterTrainMS.R'))
source(file.path(mywd, 'Functions', 'ApplyGBMKnockoff.R'))
source(file.path(mywd, 'Functions', 'HelperFunctions.R'))
source(file.path(mywd, 'Functions Dai', 'knockoff.R'))
source(file.path(mywd, 'Functions Dai', 'analysis.R'))
source(file.path(mywd, 'Functions Dai', 'MBHq.R'))
source(file.path(mywd, 'Functions Dai', 'DS.R'))
source(file.path(mywd, 'Functions Dai', 'fdp_power.R'))
lapply(c('xgboost','gbm','ranger','MASS','glmnet','knockoff','mvtnorm','hdi'),
library, character.only = TRUE)
})
registerDoParallel(cl)
# === RUN IN PARALLEL AND WRITE OUT ===
results_list <- foreach(
k = seq_len(nrow(param_grid)),
.packages = pkgs,
.combine  = rbind
) %dopar% {
s_val <- param_grid$s[k]
i_val <- param_grid$i[k]
# compute chunk of results
chunk <- Compare_SignalStrength(i = i_val, s = s_val)
# write out this chunk immediately
fname <- sprintf("Results_s%02d_i%02d.csv", s_val, i_val)
write.csv(chunk, file = paste0(mywd,"/Temp/",fname), row.names = FALSE)
# return for final binding
chunk
}
