# Consistent column names
names(X_train) <- names(X_val) <- names(X_test) <- paste0("V", seq_len(ncol(X_train)))
# Helper: R^2
R2 <- function(y_true, y_pred) 1 - sum((y_true - y_pred)^2) / sum((y_true - mean(y_true))^2)
# ======================================================================
# 1) GAM (mgcv) with shrinkage smooths; tuning k_basis and #features
# ======================================================================
library(mgcv)
# Nonlinear screening on TRAIN ONLY
dcor_screen <- function(Xdf, y) {
if (requireNamespace("energy", quietly = TRUE)) {
sapply(Xdf, function(z) energy::dcor(z, y))
} else {
# Fallback: correlation with squared predictor (captures quadratic signal)
sapply(Xdf, function(z) abs(suppressWarnings(cor(z^2, y))))
}
}
scores  <- dcor_screen(X_train, y_train)
ranked  <- order(scores, decreasing = TRUE)
k_basis_grid <- c(3, 4)
keep_grid    <- c(10, 20, 30)  # how many predictors to keep
best_gam <- list(score = -Inf)
for (k_basis in k_basis_grid) {
for (k_keep in keep_grid) {
keep_idx <- ranked[seq_len(min(k_keep, ncol(X_train)))]
Xtr <- X_train[, keep_idx, drop = FALSE]
Xv  <- X_val[,   keep_idx, drop = FALSE]
dat_tr <- data.frame(y = y_train, Xtr)
terms  <- paste(sprintf('s(%s, k=%d, bs="ts")', names(Xtr), k_basis), collapse = " + ")
form   <- as.formula(paste("y ~", terms))
fit <- gam(form, data = dat_tr, method = "REML", select = TRUE)
pred_val <- predict(fit, newdata = Xv, type = "response")
r2_val <- R2(y_val, pred_val)
if (is.finite(r2_val) && r2_val > best_gam$score) {
best_gam <- list(score = r2_val, k_basis = k_basis, k_keep = k_keep,
keep_idx = keep_idx, fit = fit)
}
}
}
# Refit GAM on train+val with best settings; evaluate on test
keep_idx <- best_gam$keep_idx
Xtrv     <- rbind(X_train[, keep_idx, drop = FALSE], X_val[, keep_idx, drop = FALSE])
y_trv    <- c(y_train, y_val)
dat_trv  <- data.frame(y = y_trv, Xtrv)
terms    <- paste(sprintf('s(%s, k=%d, bs="ts")', names(Xtrv), best_gam$k_basis), collapse = " + ")
form     <- as.formula(paste("y ~", terms))
fit_gam_final <- gam(form, data = dat_trv, method = "REML", select = TRUE)
pred_test_gam <- predict(fit_gam_final, newdata = X_test[, keep_idx, drop = FALSE])
R2_test_gam   <- R2(y_test, pred_test_gam)
# ======================================================================
# 2) SVR with degree-2 polynomial kernel; tuning hyperparameters
# ======================================================================
library(e1071)
gamma_grid <- c(0.5 / ncol(X_train), 1 / ncol(X_train), 2 / ncol(X_train))
coef0_grid <- c(0, 1)
cost_grid  <- c(0.1, 1, 10, 100)
eps_grid   <- c(0.01, 0.1, 0.3)
best_svr <- list(score = -Inf)
for (g in gamma_grid) {
for (c0 in coef0_grid) {
for (C in cost_grid) {
for (eps in eps_grid) {
fit <- svm(x = X_train, y = y_train,
type = "eps-regression",
kernel = "polynomial",
degree = 2,
gamma = g, coef0 = c0,
cost = C, epsilon = eps,
scale = TRUE)
pred_val <- predict(fit, X_val)
r2_val <- R2(y_val, pred_val)
if (is.finite(r2_val) && r2_val > best_svr$score) {
best_svr <- list(score = r2_val, gamma = g, coef0 = c0,
cost = C, epsilon = eps, fit = fit)
}
}
}
}
}
# Refit SVR on train+val with best hyperparams; evaluate on test
svr_final <- svm(x = rbind(X_train, X_val),
y = c(y_train, y_val),
type   = "eps-regression",
kernel = "polynomial",
degree = 2,
gamma  = best_svr$gamma,
coef0  = best_svr$coef0,
cost   = best_svr$cost,
epsilon= best_svr$epsilon,
scale  = TRUE)
pred_test_svr <- predict(svr_final, X_test)
R2_test_svr   <- R2(y_test, pred_test_svr)
# ---------------------- Results ---------------------------------------
cat(sprintf("GAM  | best: k_basis=%d, kept_vars=%d | Val R2=%.3f | Test R2=%.3f\n",
best_gam$k_basis, best_gam$k_keep, best_gam$score, R2_test_gam))
cat(sprintf("SVR2 | best: gamma=%.4g, coef0=%g, cost=%g, epsilon=%g | Val R2=%.3f | Test R2=%.3f\n",
best_svr$gamma, best_svr$coef0, best_svr$cost, best_svr$epsilon, best_svr$score, R2_test_svr))
X_train <- as.data.frame(X[train_id, , drop = FALSE])
X_val   <- as.data.frame(X[val_id,   , drop = FALSE])
X_test  <- as.data.frame(X[test_id,  , drop = FALSE])
y_train <- y[train_id]; y_val <- y[val_id]; y_test <- y[test_id]
# Consistent column names
# --- assumes you already have: X_train, X_val, X_test, y_train, y_val, y_test ---
R2 <- function(y, yhat) 1 - sum((y - yhat)^2) / sum((y - mean(y))^2)
# Standardize using train stats (good practice for kernels)
scale_fit  <- function(X) list(mu = colMeans(X), sd = pmax(apply(X,2,sd), 1e-8))
scale_apply<- function(X, s) sweep(sweep(X, 2, s$mu, "-"), 2, s$sd, "/")
sf <- scale_fit(X_train)
Xt  <- scale_apply(X_train, sf)
Xv  <- scale_apply(X_val,   sf)
Xs  <- scale_apply(X_test,  sf)
polyK <- function(A, B, gamma, coef0, degree = 2) (gamma * (as.matrix(A) %*% t(as.matrix(B))) + coef0)^degree
p <- ncol(X_train)
gamma_grid <- c(0.5/p, 1/p, 2/p)
coef0_grid <- c(0, 1)
lambda_grid <- 10^seq(-6, 2, length.out = 9)
best <- list(r2 = -Inf)
for (g in gamma_grid) for (c0 in coef0_grid) for (lam in lambda_grid) {
Ktr <- polyK(Xt, Xt, g, c0)
# Solve (K + lam I) alpha = y  (ridge in RKHS)
alpha <- solve(Ktr + lam * diag(nrow(Ktr)), y_train)
Kv   <- polyK(Xv, Xt, g, c0)
yhat_v <- Kv %*% alpha
r2v <- R2(y_val, yhat_v)
if (is.finite(r2v) && r2v > best$r2) best <- list(r2=r2v, gamma=g, c0=c0, lambda=lam)
}
# Refit on train+val, evaluate on test
Xtv <- rbind(Xt, Xv); y_tv <- c(y_train, y_val)
Ktv <- polyK(Xtv, Xtv, best$gamma, best$c0)
alpha_tv <- solve(Ktv + best$lambda * diag(nrow(Ktv)), y_tv)
Kte <- polyK(Xs, Xtv, best$gamma, best$c0)
yhat_te_krr <- as.vector(Kte %*% alpha_tv)
R2_test_krr <- R2(y_test, yhat_te_krr)
cat(sprintf("KRR(poly d=2) | gamma=%.4g coef0=%g lambda=%.2e | Val R2=%.3f | Test R2=%.3f\n",
best$gamma, best$c0, best$lambda, best$r2, R2_test_krr))
n <- nrow(X)
idx <- sample(seq_len(n))
n_train <- floor(n/3)
n_test  <- floor(n/3)
n_val   <- n - n_train - n_test
train_id <- idx[1:n_train]
val_id   <- idx[(n_train + 1):(n_train + n_val)]
test_id  <- idx[(n_train + n_val + 1):n]
X_train <- as.data.frame(X[train_id, , drop = FALSE])
X_val   <- as.data.frame(X[val_id,   , drop = FALSE])
X_test  <- as.data.frame(X[test_id,  , drop = FALSE])
y_train <- y[train_id]; y_val <- y[val_id]; y_test <- y[test_id]
# Consistent column names
# --- assumes you already have: X_train, X_val, X_test, y_train, y_val, y_test ---
R2 <- function(y, yhat) 1 - sum((y - yhat)^2) / sum((y - mean(y))^2)
# Standardize using train stats (good practice for kernels)
scale_fit  <- function(X) list(mu = colMeans(X), sd = pmax(apply(X,2,sd), 1e-8))
scale_apply<- function(X, s) sweep(sweep(X, 2, s$mu, "-"), 2, s$sd, "/")
sf <- scale_fit(X_train)
Xt  <- scale_apply(X_train, sf)
Xv  <- scale_apply(X_val,   sf)
Xs  <- scale_apply(X_test,  sf)
polyK <- function(A, B, gamma, coef0, degree = 2) (gamma * (as.matrix(A) %*% t(as.matrix(B))) + coef0)^degree
p <- ncol(X_train)
gamma_grid <- c(0.5/p, 1/p, 2/p)
coef0_grid <- c(0, 1)
lambda_grid <- 10^seq(-6, 2, length.out = 9)
best <- list(r2 = -Inf)
for (g in gamma_grid) for (c0 in coef0_grid) for (lam in lambda_grid) {
Ktr <- polyK(Xt, Xt, g, c0)
# Solve (K + lam I) alpha = y  (ridge in RKHS)
alpha <- solve(Ktr + lam * diag(nrow(Ktr)), y_train)
Kv   <- polyK(Xv, Xt, g, c0)
yhat_v <- Kv %*% alpha
r2v <- R2(y_val, yhat_v)
if (is.finite(r2v) && r2v > best$r2) best <- list(r2=r2v, gamma=g, c0=c0, lambda=lam)
}
# Refit on train+val, evaluate on test
Xtv <- rbind(Xt, Xv); y_tv <- c(y_train, y_val)
Ktv <- polyK(Xtv, Xtv, best$gamma, best$c0)
alpha_tv <- solve(Ktv + best$lambda * diag(nrow(Ktv)), y_tv)
Kte <- polyK(Xs, Xtv, best$gamma, best$c0)
yhat_te_krr <- as.vector(Kte %*% alpha_tv)
R2_test_krr <- R2(y_test, yhat_te_krr)
cat(sprintf("KRR(poly d=2) | gamma=%.4g coef0=%g lambda=%.2e | Val R2=%.3f | Test R2=%.3f\n",
best$gamma, best$c0, best$lambda, best$r2, R2_test_krr))
dtrain <- xgb.DMatrix(data = as.matrix(X_train), label = y_train)
dval   <- xgb.DMatrix(data = as.matrix(X_val),   label = y_val)
dtest  <- xgb.DMatrix(data = as.matrix(X_test),  label = y_test)
# Interaction constraints: one group per feature â‡’ additive model
p <- ncol(X_train)
ics_json <- toJSON(lapply(0:(p-1), function(j) list(j)), auto_unbox = TRUE)
toJSON
# Interaction constraints: one group per feature â‡’ additive model
library(xgboost); library(jsonlite)
p <- ncol(X_train)
ics_json <- toJSON(lapply(0:(p-1), function(j) list(j)), auto_unbox = TRUE)
grid <- expand.grid(
eta = c(0.01, 0.03),
max_depth = 3,
min_child_weight = c(5, 10, 20),
subsample = c(0.7, 0.9),
colsample_bytree = c(0.8, 1.0),
gamma = c(1, 3),
lambda = c(5, 10),
alpha = c(5, 25),
KEEP.OUT.ATTRS = FALSE
)
best <- list(r2 = -Inf)
for (i in seq_len(nrow(grid))) {
pars <- c(as.list(grid[i, ]), list(
objective = "reg:squarederror",
eval_metric = "rmse",
interaction_constraints = ics_json,
tree_method = "exact"
))
w <- xgb.train(
params = pars, data = dtrain,
nrounds = 2000, watchlist = list(val = dval),
early_stopping_rounds = 50, verbose = 0
)
pred_v <- predict(w, dval)
r2v <- R2(y_val, pred_v)
if (is.finite(r2v) && r2v > best$r2) best <- c(list(r2 = r2v, nrounds = w$best_iteration), grid[i, ])
}
# Refit on train+val with best hyperparams
dtrv <- xgb.DMatrix(data = as.matrix(rbind(X_train, X_val)),
label = c(y_train, y_val))
pars_best <- c(as.list(best), list(
objective = "reg:squarederror", eval_metric = "rmse",
interaction_constraints = ics_json, tree_method = "exact"
))
w_final <- xgb.train(params = pars_best, data = dtrv,
nrounds = best$nrounds, verbose = 0)
pred_te <- predict(w_final, dtest)
R2_test_xgb <- R2(y_test, pred_te)
cat(sprintf(
"XGB(additive) | eta=%.3f depth=%d mcw=%d sub=%.1f col=%.1f gamma=%d lambda=%d alpha=%d | Val R2=%.3f | Test R2=%.3f\n",
best$eta, best$max_depth, best$min_child_weight, best$subsample,
best$colsample_bytree, best$gamma, best$lambda, best$alpha, best$r2, R2_test_xgb))
# --- Split: 1/3 train, 1/3 val, 1/3 test -------------------------------
# --- Split: 1/3 train, 1/3 val, 1/3 test --------------------------------
n <- nrow(X)
idx <- sample(seq_len(n))
n_train <- floor(n/3)
n_test  <- floor(n/3)
n_val   <- n - n_train - n_test
train_id <- idx[1:n_train]
val_id   <- idx[(n_train + 1):(n_train + n_val)]
test_id  <- idx[(n_train + n_val + 1):n]
# DMatrix
X_train <- as.matrix(X[train_id, , drop = FALSE])
X_val   <- as.matrix(X[val_id,   , drop = FALSE])
X_test  <- as.matrix(X[test_id,  , drop = FALSE])
y_train <- y[train_id]; y_val <- y[val_id]; y_test <- y[test_id]
# Standardize predictors using TRAIN stats (helps tree proposals with high-p)
scale_fit   <- function(A) list(mu = colMeans(A), sd = pmax(apply(A, 2, sd), 1e-8))
scale_apply <- function(A, s) sweep(sweep(A, 2, s$mu, "-"), 2, s$sd, "/")
sf <- scale_fit(X_train)
Xt <- scale_apply(X_train, sf)
Xv <- scale_apply(X_val,   sf)
Xs <- scale_apply(X_test,  sf)
# --- BART tuning ---------------------------------------------------------
suppressPackageStartupMessages(library(dbarts))
# --- Split: 1/3 train, 1/3 val, 1/3 test -------------------------------
# --- Split: 1/3 train, 1/3 val, 1/3 test --------------------------------
n <- nrow(X)
idx <- sample(seq_len(n))
n_train <- floor(n/3)
n_test  <- floor(n/3)
n_val   <- n - n_train - n_test
train_id <- idx[1:n_train]
val_id   <- idx[(n_train + 1):(n_train + n_val)]
test_id  <- idx[(n_train + n_val + 1):n]
# DMatrix
X_train <- as.matrix(X[train_id, , drop = FALSE])
X_val   <- as.matrix(X[val_id,   , drop = FALSE])
X_test  <- as.matrix(X[test_id,  , drop = FALSE])
y_train <- y[train_id]; y_val <- y[val_id]; y_test <- y[test_id]
# Standardize predictors using TRAIN stats (helps tree proposals with high-p)
scale_fit   <- function(A) list(mu = colMeans(A), sd = pmax(apply(A, 2, sd), 1e-8))
scale_apply <- function(A, s) sweep(sweep(A, 2, s$mu, "-"), 2, s$sd, "/")
sf <- scale_fit(X_train)
Xt <- scale_apply(X_train, sf)
Xv <- scale_apply(X_val,   sf)
Xs <- scale_apply(X_test,  sf)
# --- BART tuning ---------------------------------------------------------
suppressPackageStartupMessages(library(dbarts))
# Modest grids (keep runtime reasonable with n=133)
ntree_grid   <- c(50, 100, 200)       # number of trees
k_grid       <- c(1, 2, 3)            # prior shrinkage (larger = stronger shrinkage)
power_grid   <- c(2.0)                # tree depth prior beta (default ~2)
base_grid    <- c(0.95)               # tree depth prior alpha (default ~0.95)
sigdf_grid   <- c(3)                  # df for sigma prior
sigquant_grid<- c(0.90, 0.99)         # quantile for sigma prior (robust vs. diffuse)
best <- list(r2 = -Inf)
for (nt in ntree_grid) {
for (k in k_grid) {
for (pw in power_grid) {
for (bs in base_grid) {
for (sdof in sigdf_grid) {
for (sq in sigquant_grid) {
fit <- dbarts::bart(
x.train = Xt, y.train = y_train,
x.test  = Xv,
ntree   = nt,
k       = k,
power   = pw,
base    = bs,
sigdf   = sdof,
sigquant= sq,
nskip   = 1000,    # burn-in
ndpost  = 1000,    # posterior draws
keeptrees = FALSE,
verbose = FALSE
)
pred_val <- as.vector(fit$yhat.test.mean)
r2_val <- R2(y_val, pred_val)
if (is.finite(r2_val) && r2_val > best$r2) {
best <- list(r2 = r2_val, ntree = nt, k = k, power = pw, base = bs,
sigdf = sdof, sigquant = sq)
}
}
}
}
}
}
}
set.seed(42)
n <-400
p <- 500
p0 <- 25
q <- 0.1
delta=10
n1 <- floor(n/2); n2 <- n - n1
X1 <- matrix(rnorm(n1*p, mean=0), n1, p)
X2 <- matrix(rnorm(n2*p, mean=0), n2, p)
X  <- rbind(X1, X2)
beta_star <- numeric(p)
signal_index <- sample(c(1:p), size = p0, replace = F)
beta_star[signal_index] <- rnorm(p0, 0, delta*sqrt(log(p)/n))*100
y <- (X^2 %*% beta_star + rnorm(n))
# --- Split: 1/3 train, 1/3 val, 1/3 test --------------------------------
n <- nrow(X)
idx <- sample(seq_len(n))
n_train <- floor(n/3)
n_test  <- floor(n/3)
n_val   <- n - n_train - n_test
train_id <- idx[1:n_train]
val_id   <- idx[(n_train + 1):(n_train + n_val)]
test_id  <- idx[(n_train + n_val + 1):n]
# DMatrix
X_train <- as.matrix(X[train_id, , drop = FALSE])
X_val   <- as.matrix(X[val_id,   , drop = FALSE])
X_test  <- as.matrix(X[test_id,  , drop = FALSE])
y_train <- y[train_id]; y_val <- y[val_id]; y_test <- y[test_id]
# Standardize predictors using TRAIN stats (helps tree proposals with high-p)
scale_fit   <- function(A) list(mu = colMeans(A), sd = pmax(apply(A, 2, sd), 1e-8))
scale_apply <- function(A, s) sweep(sweep(A, 2, s$mu, "-"), 2, s$sd, "/")
sf <- scale_fit(X_train)
Xt <- scale_apply(X_train, sf)
Xv <- scale_apply(X_val,   sf)
Xs <- scale_apply(X_test,  sf)
# --- BART tuning ---------------------------------------------------------
suppressPackageStartupMessages(library(dbarts))
# Modest grids (keep runtime reasonable with n=133)
ntree_grid   <- c(50, 100, 200)       # number of trees
k_grid       <- c(1, 2, 3)            # prior shrinkage (larger = stronger shrinkage)
power_grid   <- c(2.0)                # tree depth prior beta (default ~2)
base_grid    <- c(0.95)               # tree depth prior alpha (default ~0.95)
sigdf_grid   <- c(3)                  # df for sigma prior
sigquant_grid<- c(0.90, 0.99)         # quantile for sigma prior (robust vs. diffuse)
best <- list(r2 = -Inf)
for (nt in ntree_grid) {
for (k in k_grid) {
for (pw in power_grid) {
for (bs in base_grid) {
for (sdof in sigdf_grid) {
for (sq in sigquant_grid) {
fit <- dbarts::bart(
x.train = Xt, y.train = y_train,
x.test  = Xv,
ntree   = nt,
k       = k,
power   = pw,
base    = bs,
sigdf   = sdof,
sigquant= sq,
nskip   = 1000,    # burn-in
ndpost  = 1000,    # posterior draws
keeptrees = FALSE,
verbose = FALSE
)
pred_val <- as.vector(fit$yhat.test.mean)
r2_val <- R2(y_val, pred_val)
if (is.finite(r2_val) && r2_val > best$r2) {
best <- list(r2 = r2_val, ntree = nt, k = k, power = pw, base = bs,
sigdf = sdof, sigquant = sq)
}
}
}
}
}
}
}
fit <- dbarts::bart(
x.train = Xt, y.train = y_train,
x.test  = Xv,
ntree   = nt,
k       = k,
power   = pw,
base    = bs,
sigdf   = sdof,
sigquant= sq,
nskip   = 1000,    # burn-in
ndpost  = 1000,    # posterior draws
keeptrees = FALSE,
verbose = FALSE
)
y_train
y_val
y_test
set.seed(42)
n <-400
p <- 500
p0 <- 25
q <- 0.1
delta=10
n1 <- floor(n/2); n2 <- n - n1
X1 <- matrix(rnorm(n1*p, mean=0), n1, p)
X2 <- matrix(rnorm(n2*p, mean=0), n2, p)
X  <- rbind(X1, X2)
beta_star <- numeric(p)
signal_index <- sample(c(1:p), size = p0, replace = F)
beta_star[signal_index] <- rnorm(p0, 0, delta*sqrt(log(p)/n))*100
y <- (X^2 %*% beta_star + rnorm(n))
# --- Split: 1/3 train, 1/3 val, 1/3 test -------------------------------
# --- Split: 1/3 train, 1/3 val, 1/3 test --------------------------------
n <- nrow(X)
idx <- sample(seq_len(n))
n_train <- floor(n/3)
n_test  <- floor(n/3)
n_val   <- n - n_train - n_test
train_id <- idx[1:n_train]
val_id   <- idx[(n_train + 1):(n_train + n_val)]
test_id  <- idx[(n_train + n_val + 1):n]
# DMatrix
X_train <- as.matrix(X[train_id, , drop = FALSE])
X_val   <- as.matrix(X[val_id,   , drop = FALSE])
X_test  <- as.matrix(X[test_id,  , drop = FALSE])
y_train <- y[train_id]; y_val <- y[val_id]; y_test <- y[test_id]
# Standardize predictors using TRAIN stats (helps tree proposals with high-p)
scale_fit   <- function(A) list(mu = colMeans(A), sd = pmax(apply(A, 2, sd), 1e-8))
scale_apply <- function(A, s) sweep(sweep(A, 2, s$mu, "-"), 2, s$sd, "/")
sf <- scale_fit(X_train)
Xt <- scale_apply(X_train, sf)
Xv <- scale_apply(X_val,   sf)
Xs <- scale_apply(X_test,  sf)
X_train
X_val
X_test
y_train
fit <- dbarts::bart(
x.train = Xt, y.train = y_train,
x.test  = Xv,
nskip   = 1000,    # burn-in
ndpost  = 1000,    # posterior draws
keeptrees = FALSE,
verbose = FALSE
)
y_train
Xt
Xv
fit <- dbarts::bart(
x.train = Xt, y.train = y_train,
nskip   = 1000,    # burn-in
ndpost  = 1000,    # posterior draws
keeptrees = FALSE,
verbose = FALSE
)
fit <- dbarts::bart(
x.train = X_train, y.train = y_train,
nskip   = 1000,    # burn-in
ndpost  = 1000,    # posterior draws
keeptrees = FALSE,
verbose = FALSE
)
fit <- dbarts::bart(
x.train = X_train, y.train = y_train,
nskip   = 1000,    # burn-in
ndpost  = 1000,    # posterior draws
keeptrees = FALSE,
verbose = FALSE
)
X_train
y_train
summary(y_train); sd(y_train)
anyNA(y_train); any(!is.finite(y_train))
colSums(!is.finite(X_train))[1:10]  # first few columns
sum(apply(X_train, 2, sd) == 0)     # zero-variance columns
wb <- wbart(x.train = Xt, y.train = yt, x.test = Xv, ndpost=1000, nskip=1000, ntree=200)
install.packages("BART")
# Inputs:
##   X: n x p numeric matrix/data.frame of predictors
##   y: length-n numeric response
## Packages needed: mgcv, e1071 (optional: energy for distance correlation)
library(BART)
wb <- wbart(x.train = Xt, y.train = yt, x.test = Xv, ndpost=1000, nskip=1000, ntree=200)
wb <- wbart(x.train = Xt, y.train = y_train, x.test = Xv, ndpost=1000, nskip=1000, ntree=200)
wb
R2(yv, wb$yhat.test.mean)
y_test
R2(y_test, wb$yhat.test.mean)
pred_test <- predict(wb,newdata=X_test)
R2_test   <- R2(y_test, pred_test)
R2_test
pred_test
