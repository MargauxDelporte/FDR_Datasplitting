# rorp_prs_norm is the only covariate (like your SAS code)
model_incid <- crr(
ftime   = df_incid$time_inc,
fstatus = df_incid$status_inc,
cov1    = matrix(df_incid$rorp_prs_norm,df_incid$prs_norm, ncol=2),
cencode = 0,
failcode = 1
)
# Fit the Fine-Gray model
# rorp_prs_norm is the only covariate (like your SAS code)
model_incid <- crr(
ftime   = df_incid$time_inc,
fstatus = df_incid$status_inc,
cov1    = matrix(c(df_incid$rorp_prs_norm,df_incid$prs_norm), ncol=2),
cencode = 0,
failcode = 1
)
matrix(c(df_incid$rorp_prs_norm,df_incid$prs_norm), ncol=2)
# Fit the Fine-Gray model
# rorp_prs_norm is the only covariate (like your SAS code)
model_incid <- crr(
ftime   = df_incid$time_inc,
fstatus = df_incid$status_inc,
cov1    = matrix(c(df_incid$rorp_prs_norm,df_incid$prs_norm), ncol=2),
cencode = 0,
failcode = 1
)
summary(model_incid)
# Fit the Fine-Gray model
# rorp_prs_norm is the only covariate (like your SAS code)
covariates <- as.matrix(data.frame(df_incid$rorp_prs_norm, df_incid$prs_norm))
model_incid <- crr(
ftime   = df_incid$time_inc,
fstatus = df_incid$status_inc,
cov1    = covariates,
cencode = 0,
failcode = 1
)
summary(model_incid)
model_incid$coef[1]
incid_stderr1_prs <- sqrt(model_incid$var)[2]  # variance -> sqrt for std error
incid_beta1_prs   <- model_incid$coef[2]
incid_stderr1_prs <- sqrt(model_incid$var)[2]  # variance -> sqrt for std error
incid_beta1_prs
incid_stderr1_prs
incid_stderr1 <- sqrt(model_incid$var[1]) # variance -> sqrt for std error
incid_stderr1
incid_beta1_prs   <- model_incid$coef[2]
incid_stderr1_prs <- sqrt(model_incid$var[2])  # variance -> sqrt for std error
incid_beta1_prs
incid_stderr1_prs
model_incid$var[2]
incid_stderr1_prs <- sqrt(model_incid$var[2])  # variance -> sqrt for std error
sqrt(model_incid$se(coef))
model_incid$se
se(coef)
model_incid$se(coef
summary(model_incid)
summary(model_incid)
list(model_incid)
model_incid$se
model_incid$var
sqrt(model_incid$var[1,1])
incid_stderr1_prs <- sqrt(model_incid$var[2,2])  # variance -> sqrt for std error
incid_stderr1_prs
covariates <- as.matrix(data.frame(df_mort$rorp_prs_norm, df_mort$prs_norm))
covariates_mort <- as.matrix(data.frame(df_mort$rorp_prs_norm, df_mort$prs_norm))
model_mort <- crr(
ftime   = df_mort$time_mort,
fstatus = df_mort$status,
cov1    = covariates_mort,
cencode = 0,
failcode = 1
)
covariates_mort <- as.matrix(data.frame(df_mort$rorp_prs_norm, df_mort$prs_norm))
df_mort <- mydata %>%
filter(!is.na(time_mort) & !is.na(status))
covariates_mort <- as.matrix(data.frame(df_mort$rorp_prs_norm, df_mort$prs_norm))
model_mort <- crr(
ftime   = df_mort$time_mort,
fstatus = df_mort$status,
cov1    = covariates_mort,
cencode = 0,
failcode = 1
)
summary(model_mort)
mort_beta1   <- model_mort$coef[1]
mort_stderr1 <- sqrt(model_mort$var[1,1])
mort_beta1
mort_stderr1
mort_beta1_prs   <- model_mort$coef[2]
mort_stderr1_prs <- sqrt(model_mort$var[2,2])
mort_beta1_prs
mort_stderr1_prs
exp(-1.29)/exp(-0.72)
exp(0.57)
exp(-0.57)
exp(0.57)
exp(-0.57)
exp(-0.57)
exp(-1.29)/exp(-0.72)
exp(-0.57)
exp(-1.29)/exp(-0.72)
library(readr)
library(xlsx)
library(openxlsx)
wd='C:/Users/mde4023/OneDrive - Weill Cornell Medicine/0 Projects/drive'
icogs_onco_meta <- read.csv("C:/Users/mde4023/OneDrive - Weill Cornell Medicine/0 Projects/drive/icogs_onco_meta.txt", sep="")
View(icogs_onco_meta)
icogs_onco_meta_rs1800437=icogs_onco_meta[which(grepl('rs1800437',icogs_onco_meta$SNP.iCOGs)),]
icogs_onco_meta_rs1800437
write.xlsx(icogs_onco_meta_rs1800437,file=paste0(wd,'icogs_onco_meta_rs1800437.xlsx'))
which(grepl('rs1800437',icogs_onco_meta$SNP.iCOGs))
vec <- c(1, 0.5, 3, 4, 5)
cummean(vec)
library(dplyr)
cummean(vec)
#j=1
#model=lm
library(keras)
# --- 2) Build the model ---
inp   <- layer_input(shape = p, name = "input")
num_split <- 10
n <-1500
p <- 250
p0 <- 25
q <- 0.1
signal_index <- sample(c(1:p), size = p0, replace = F)
i=10
delta <- i
# simulate data
X <- mvrnorm(n, mu = rep(0, p), Sigma = diag(p))
powers=sample(size=p,c(-2:-1,1:2),replace=T)
beta_star <- numeric(p)
beta_star
beta_star[signal_index] <- rnorm(p0, 0, delta*sqrt(log(p)/n))*10
y <- scale(X^2 %*% beta_star + rnorm(n))
# simulate data
X <- mvrnorm(n, mu = rep(0, p), Sigma = diag(p))
library(MASS)
# simulate data
X <- mvrnorm(n, mu = rep(0, p), Sigma = diag(p))
beta_star <- numeric(p)
beta_star[signal_index] <- rnorm(p0, 0, delta*sqrt(log(p)/n))*10
y <- scale(X^2 %*% beta_star + rnorm(n))
# --- 2) Build the model ---
inp   <- layer_input(shape = p, name = "input")
library(tensorflow)
# --- 2) Build the model ---
inp   <- layer_input(shape = p, name = "input")
library(reticulate)
use_condaenv("r-tensorflow", required = TRUE)
conda_install(
envname  = "r-tensorflow",
packages = c("tensorflow","keras","h5py"),
channel   = "conda-forge",  # often more up‑to‑date builds
pip       = FALSE
)
reticulate::miniconda_remove()
reticulate::miniconda_remove()
library(reticulate)
reticulate::miniconda_remove()
#j=1
library(keras)
library(tensorflow)
library(reticulate)
conda_list()
use_condaenv("tensorF", required = TRUE)
use_condaenv("tensorF", required = TRUE)
#j=1
library(keras)
library(tensorflow)
install.packages("keras3")
install.packages("tensorflow")
install.packages("reticulate")
#j=1
library(keras3)
library(tensorflow)
model <- keras_model_sequential()
#j=1
library(keras)
library(keras3)
#j=1
library(keras3)
library(tensorflow)
model <- keras_model_sequential()
#j=1
reticulate::py_config()
library(reticulate)
use_virtualenv("r-tf-venv", required = TRUE)
use_virtualenv("r-tf", required = TRUE)
use_virtualenv("r-tf", required = TRUE)
library(reticulate)
use_python("C:/…/tf-env/python.exe", required=TRUE)
use_python("C:/…/tf/python.exe", required=TRUE)
use_python("C:/…/tf/python.exe", required=TRUE)
py <- py_discover_config()$python
virtualenv_remove("r-tf-venv", confirm = FALSE)  # clean up any broken one
virtualenv_create("r-tf-venv", python = py)
virtualenv_install("r-tf-venv",
packages = c("tensorflow-cpu>=2.18.0","keras","h5py"))
use_virtualenv("r-tf-venv", required = TRUE)
library(tensorflow)
# install.packages("tensorflow")
tensorflow::install_tensorflow()  # if you haven't yet
library(reticulate)
reticulate::install_python(version = '3.10')
tensorflow::install_tensorflow()
# install.packages("tensorflow")
tensorflow::install_tensorflow()  # if you haven't yet
# install.packages("tensorflow")
tensorflow::install_tensorflow()  # if you haven't yet
library(reticulate)
use_condaenv("r-tf-py310", required = TRUE)
library(reticulate)
use_condaenv("r-tf-py310", required = TRUE)
reticulate::install_miniconda()
reticulate::conda_create("r-tf-py310", python_version = "3.10")
reticulate::use_condaenv("r-tf-py310", required = TRUE)
library(tensorflow)
tensorflow::install_tensorflow(
envname = "r-tf-py310",
version = "2.16.2",
method  = "conda"
)
#j=1
library(keras)
library(tensorflow)
use_virtualenv("r-tf-py310", required = TRUE)
model <- keras_model_sequential() %>%
# 1) square every input feature
layer_lambda(
f         = function(x) k_square(x),
input_shape = p,
name      = "quadratic_layer"
) %>%
# 2) linear regression on the squared features
layer_dense(
units      = 1,
activation = "linear",
name       = "output"
)
use_condaenv("r-tf310", required = TRUE)
library(reticulate)
use_condaenv("r-tf310", required = TRUE)
reticulate::conda_list()
use_condaenv("r-tf-py310", required = TRUE)
### High dimension linear model
rm(list = ls())
mywd='C:/Users/mde4023/OneDrive - Weill Cornell Medicine/0 Projects/FDR_Datasplitting'
#mywd='C:/Users/mde4023/Documents/GitHub/FDR_Datasplitting'
setwd(mywd)
source(paste0(mywd,'/Functions/HelperFunctions.R'))
source(paste0(mywd,'/Functions/TriangleLinRegTrainMS.R'))
source(paste0(mywd,'/Functions Dai/knockoff.R'))
source(paste0(mywd,'/Functions Dai/analysis.R'))
source(paste0(mywd,'/Functions Dai/MBHq.R'))
source(paste0(mywd,'/Functions Dai/DS.R'))
source(paste0(mywd,'/Functions Dai/fdp_power.R'))
#devtools::install_github("Jeremy690/DSfdr/DSfdr",force = TRUE)
library(MASS)
library(glmnet)
library(knockoff)
library(mvtnorm)
library(hdi)
library(dplyr)
library(ggplot2)
library(ggpubr)
### algorithmic settings
num_split <- 10
n <-1500
p <- 250
p0 <- 25
q <- 0.1
#set.seed(124)(123) i=5
set.seed(456)
signal_index <- sample(c(1:p), size = p0, replace = F)
################################Triange: train test test each 30####################################
Compare_SignalStrength=function(i,s){
set.seed(s)
ResultsDataFrame=data.frame()
delta <- i
X <- mvrnorm(n, mu = rep(0, p), Sigma = diag(p))
### randomly generate the true beta i=4
beta_star <- rep(0, p)
beta_star[signal_index] <- rnorm(p0, mean = 0, sd = delta*sqrt(log(p)/n))
### generate y
y <- X%*%beta_star + rnorm(n, mean = 0, sd = 1)
###my own methods:
g=ApplyTriangleLinRegTrain(X=as.data.frame(X), y, q=0.1,num_split=10, signal_index=signal_index, amountTrain=0.333, myseed = 1)
ResultsDataFrame=c('LinReg DS',i, as.numeric(g$DS_fdp),as.numeric(g$DS_power))
ResultsDataFrame=rbind(ResultsDataFrame,c('LinReg MS',i, as.numeric(g$MDS_fdp),as.numeric(g$MDS_power)))
### Competition
DS_result <- DS(X,y, num_split=10, q=0.1)
ResultsDataFrame=rbind(ResultsDataFrame,c('DataSplitting',i,DS_result$DS_fdp,DS_result$DS_power))
ResultsDataFrame=rbind(ResultsDataFrame,c('MultipleDataSplitting',i,DS_result$MDS_fdp,DS_result$MDS_power))
knockoff_result <- knockoff(X, y, q=0.1)
ResultsDataFrame=rbind(ResultsDataFrame,c('Knockoff',i,knockoff_result$fdp,knockoff_result$power))
BH_result <- MBHq(X, y, q=0.1, num_split)
ResultsDataFrame=rbind(ResultsDataFrame,c('BH',i,BH_result$fdp,BH_result$power))
### save data
return(ResultsDataFrame)}
Compare_SignalStrenght(7,1)
library(parallel)
mywd <- 'C:/Users/mde4023/OneDrive - Weill Cornell Medicine/0 Projects/FDR_Datasplitting'
setwd(mywd)
# Source helper and method files
source(file.path(mywd, 'Functions','HelperFunctions.R'))
source(file.path(mywd, 'Functions', 'TriangleLinRegTrainMS.R'))
# Dai’s routines
source(file.path(mywd, 'Functions Dai', 'knockoff.R'))
source(file.path(mywd, 'Functions Dai', 'analysis.R'))
source(file.path(mywd, 'Functions Dai', 'MBHq.R'))
source(file.path(mywd, 'Functions Dai', 'DS.R'))
source(file.path(mywd, 'Functions Dai', 'fdp_power.R'))
# Load required packages
pkgs <- c('xgboost','gbm','ranger','MASS','glmnet','knockoff','mvtnorm','hdi',
'foreach','doParallel')
lapply(pkgs, library, character.only = TRUE)
# === PARAMETER GRID ===
param_grid <- expand.grid(
s = 1:50,
i = seq(from = 7, to = 13, by = 1)
)
# === SET UP PARALLEL BACKEND ===
cl <- makeCluster(detectCores() - 4)
# export working dir so workers can source
clusterExport(cl, 'mywd')
# have each worker source & load libraries
clusterEvalQ(cl, {
setwd(mywd)
source(file.path(mywd, 'Functions', 'HelperFunctions.R'))
source(file.path(mywd, 'Functions', 'TriangleLinRegTrainMS.R'))
source(file.path(mywd, 'Functions Dai', 'knockoff.R'))
source(file.path(mywd, 'Functions Dai', 'analysis.R'))
source(file.path(mywd, 'Functions Dai', 'MBHq.R'))
source(file.path(mywd, 'Functions Dai', 'DS.R'))
source(file.path(mywd, 'Functions Dai', 'fdp_power.R'))
lapply(c('xgboost','gbm','ranger','MASS','glmnet','knockoff','mvtnorm','hdi'),
library, character.only = TRUE)
})
registerDoParallel(cl)
# === RUN IN PARALLEL AND WRITE OUT ===
results_list <- foreach(
k = seq_len(nrow(param_grid)),
.packages = pkgs,
.combine  = rbind
) %dopar% {
s_val <- param_grid$s[k]
i_val <- param_grid$i[k]
# compute chunk of results
chunk <- Compare_SignalStrength(i = i_val, s = s_val)
# write out this chunk immediately
fname <- sprintf("Results_s%02d_i%02d.csv", s_val, i_val)
write.csv(chunk, file = fname, row.names = FALSE)
# return for final binding
chunk
}
# === CLEANUP AND FINAL SAVE ===
stopCluster(cl)
warnings()
# combine all and save full dataset
Results <- results_list
write.csv(Results, file = "All_Results.csv", row.names = FALSE)
Results
### High dimension linear model
rm(list = ls())
mywd='C:/Users/mde4023/OneDrive - Weill Cornell Medicine/0 Projects/FDR_Datasplitting'
#mywd='C:/Users/mde4023/Documents/GitHub/FDR_Datasplitting'
setwd(mywd)
source('HelperFunctions.R')
### High dimension linear model
rm(list = ls())
mywd='C:/Users/mde4023/OneDrive - Weill Cornell Medicine/0 Projects/FDR_Datasplitting'
setwd(mywd)
source('HelperFunctions.R')
source(paste0(mywd,'/Functions/HelperFunctions.R'))
source(paste0(mywd,'/Functions/TriangleBoosterTrainMS.R'))
source(paste0(mywd,'/Functions/TriangleRangerTrainMS.R'))
source(paste0(mywd,'/Functions/TriangleGBMTrainMS.R'))
source(paste0(mywd,'/Functions Dai/knockoff.R'))
source(paste0(mywd,'/Functions Dai/analysis.R'))
source(paste0(mywd,'/Functions Dai/MBHq.R'))
source(paste0(mywd,'/Functions Dai/DS.R'))
source(paste0(mywd,'/Functions Dai/fdp_power.R'))
#devtools::install_github("Jeremy690/DSfdr/DSfdr",force = TRUE)
library(xgboost)
library(gbm)
library(ranger)
library(MASS)
library(neuralnet)
library(glmnet)
library(knockoff)
library(mvtnorm)
library(hdi)
### algorithmic settings
num_split <- 10
n <-7500
p <- 250
p0 <- 25
q <- 0.1
#set.seed(124)(123) i=5
set.seed(456)
signal_index <- sample(c(1:p), size = p0, replace = F)
#######set up the method for the comparison############# i=10
Compare_SignalStrength <- function(i, s) {
set.seed(s)
delta <- i
# simulate data
X <- mvrnorm(n, mu = rep(0, p), Sigma = diag(p))
beta_star <- numeric(p)
beta_star[signal_index] <- rnorm(p0, 0, delta*sqrt(log(p)/n))*10
y <- scale(X^2 %*% beta_star + rnorm(n))
# run your custom methods
g1 <- ApplyTriangleBoostTrain( X = X, y = y, q = q, num_split = num_split,
signal_index = signal_index, myseed = 1)
# g2 <- ApplyTriangleGBMTrain(   X = X, y = y, q = q, num_split = num_split,
#                                signal_index = signal_index, myseed = 1)
# g3 <- ApplyTriangleRangerTrain(X = X, y = y, q = q, num_split = num_split,
#                               signal_index = signal_index, myseed = 1)
# FDR methods
DS_result      <- DS(          X = X, y = y, q = q, num_split = num_split)
knockoff_result<- knockoff(    X = X, y = y, q = q)
BH_result      <- MBHq(        X = X, y = y, q = q, num_split = num_split)
# init empty results df
ResultsDataFrame <- data.frame(
Method = character(),
Delta  = numeric(),
FDP    = numeric(),
Power  = numeric(),
stringsAsFactors = FALSE
)
# bind all rows
ResultsDataFrame <- rbind(
ResultsDataFrame,
data.frame(Method = "Boost DS",                Delta = i, FDP = g1$DS_fdp,    Power = g1$DS_power),
data.frame(Method = "Boost MS",                Delta = i, FDP = g1$MDS_fdp,   Power = g1$MDS_power),
#data.frame(Method = "GBM DS",                  Delta = i, FDP = g2$DS_fdp,    Power = g2$DS_power),
#data.frame(Method = "GBM MS",                  Delta = i, FDP = g2$MDS_fdp,   Power = g2$MDS_power),
#data.frame(Method = "Ranger DS",               Delta = i, FDP = g3$DS_fdp,    Power = g3$DS_power),
#data.frame(Method = "Ranger MS",               Delta = i, FDP = g3$MDS_fdp,   Power = g3$MDS_power),
data.frame(Method = "DataSplitting",           Delta = i, FDP = DS_result$DS_fdp,  Power = DS_result$DS_power),
data.frame(Method = "MultipleDataSplitting",   Delta = i, FDP = DS_result$MDS_fdp, Power = DS_result$MDS_power),
data.frame(Method = "Knockoff",                Delta = i, FDP = knockoff_result$fdp, Power = knockoff_result$power),
data.frame(Method = "Benjamini–Hochberg (BH)", Delta = i, FDP = BH_result$fdp,     Power = BH_result$power)
)
return(ResultsDataFrame)
}
Compare_SignalStrength(1,5)
library(parallel)
mywd <- 'C:/Users/mde4023/OneDrive - Weill Cornell Medicine/0 Projects/FDR_Datasplitting'
setwd(mywd)
source(file.path(mywd, 'Functions', 'TriangleBoosterTrainMS.R'))
source(file.path(mywd, 'Functions', 'HelperFunctions.R'))
# Dai’s routines
source(file.path(mywd, 'Functions Dai', 'knockoff.R'))
source(file.path(mywd, 'Functions Dai', 'analysis.R'))
source(file.path(mywd, 'Functions Dai', 'MBHq.R'))
source(file.path(mywd, 'Functions Dai', 'DS.R'))
source(file.path(mywd, 'Functions Dai', 'fdp_power.R'))
# Load required packages
pkgs <- c('xgboost','gbm','ranger','MASS','glmnet','knockoff','mvtnorm','hdi',
'foreach','doParallel')
lapply(pkgs, library, character.only = TRUE)
# === PARAMETER GRID ===
param_grid <- expand.grid(
s = 26:50,
i = seq(from = 7, to = 13, by = 1)
)
# === SET UP PARALLEL BACKEND ===
cl <- makeCluster(detectCores() - 4)
# export working dir so workers can source
clusterExport(cl, 'mywd')
# have each worker source & load libraries
clusterEvalQ(cl, {
setwd(mywd)
source(file.path(mywd, 'Functions', 'TriangleBoosterTrainMS.R'))
source(file.path(mywd, 'Functions', 'HelperFunctions.R'))
source(file.path(mywd, 'Functions Dai', 'knockoff.R'))
source(file.path(mywd, 'Functions Dai', 'analysis.R'))
source(file.path(mywd, 'Functions Dai', 'MBHq.R'))
source(file.path(mywd, 'Functions Dai', 'DS.R'))
source(file.path(mywd, 'Functions Dai', 'fdp_power.R'))
lapply(c('xgboost','gbm','ranger','MASS','glmnet','knockoff','mvtnorm','hdi'),
library, character.only = TRUE)
})
registerDoParallel(cl)
# === RUN IN PARALLEL AND WRITE OUT ===
results_list <- foreach(
k = seq_len(nrow(param_grid)),
.packages = pkgs,
.combine  = rbind
) %dopar% {
s_val <- param_grid$s[k]
i_val <- param_grid$i[k]
# compute chunk of results
chunk <- Compare_SignalStrength(i = i_val, s = s_val)
# write out this chunk immediately
fname <- sprintf("Results_s%02d_i%02d.csv", s_val, i_val)
write.csv(chunk, file = fname, row.names = FALSE)
# return for final binding
chunk
}
# === CLEANUP AND FINAL SAVE ===
stopCluster(cl)
warnings()
# combine all and save full dataset
Results <- results_list
write.csv(Results, file = "All_Results.csv", row.names = FALSE)
Results
write.csv(Results, file = "All_Results.csv", row.names = FALSE)
write.csv(Results, file = "NonlinearScenario_10x_seed26_50.csv", row.names = FALSE)
